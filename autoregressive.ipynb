{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from data_rnn import load_ndfa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import torch.distributions as dist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "n=150000\n",
    "x_train, (i2w, w2i) = load_ndfa(n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary:{'.pad': 0, '.start': 1, '.end': 2, '.unk': 3, '!': 4, 'v': 5, 'b': 6, 'u': 7, 'k': 8, 's': 9, 'c': 10, 'm': 11, 'a': 12, 'l': 13, 'w': 14}\n",
      "Index:['.pad', '.start', '.end', '.unk', '!', 'v', 'b', 'u', 'k', 's', 'c', 'm', 'a', 'l', 'w']\n"
     ]
    }
   ],
   "source": [
    "print(f'Dictionary:{w2i}')\n",
    "print(f'Index:{i2w}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def print_sequence(x_train, i):\n",
    "    print(f\"Sequence #{str(i).rjust(6, ' ')}: {''.join([i2w[i] for i in x_train[i]])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence #132514: sabc!abc!abc!abc!abc!abc!abc!s\n",
      "Sequence #  8670: ss\n",
      "Sequence #  2504: ss\n",
      "Sequence #110868: sabc!abc!abc!abc!s\n",
      "Sequence # 98074: sklm!klm!klm!s\n",
      "Sequence # 52135: sklm!s\n",
      "Sequence # 62973: sabc!s\n",
      "Sequence # 67071: suvw!uvw!s\n",
      "Sequence # 16084: ss\n",
      "Sequence #123280: sklm!klm!klm!klm!klm!s\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.randint(n, size=10):\n",
    "    print_sequence(x_train, i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "[9, 12, 6, 10, 4, 12, 6, 10, 4, 9]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[74191]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "vocab_size=len(w2i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "hidden_size = 16\n",
    "lstm_num_layers = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 hidden_size,\n",
    "                 lstm_num_layers) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, lstm_num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = self.embed(x)\n",
    "        lstm_output, (hn, cn) = self.lstm(input)\n",
    "        output = self.linear(lstm_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "net =  Net(vocab_size, embedding_size, hidden_size, lstm_num_layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "Net(\n  (embed): Embedding(15, 32)\n  (lstm): LSTM(32, 16, batch_first=True)\n  (linear): Linear(in_features=16, out_features=15, bias=True)\n)"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def batch_length(batch):\n",
    "    return max(len(seq) for seq in batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def add_padding(seq, amt=1):\n",
    "    for _ in range(amt):\n",
    "        seq.append(w2i['.pad'])\n",
    "    return seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "def add_start(seq):\n",
    "    seq.insert(0, w2i['.start'])\n",
    "    return seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def add_end(seq):\n",
    "    seq.append(w2i['.end'])\n",
    "    return seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "def preprocess_batch(batch):\n",
    "    max_len = batch_length(batch)\n",
    "\n",
    "    upd_batch = []\n",
    "    for i, _ in enumerate(batch):\n",
    "        seq = batch[i].copy()\n",
    "        seq = add_start(seq)\n",
    "        seq = add_end(seq)\n",
    "        seq = add_padding(seq, amt=max_len + 2 - len(seq))\n",
    "        upd_batch.append(seq)\n",
    "\n",
    "    upd_batch = torch.tensor(upd_batch, dtype=torch.long)\n",
    "    targets = upd_batch.clone().detach()\n",
    "    # Remove first column of tensor\n",
    "    targets = targets[:, 1:]\n",
    "    # Append a zero column\n",
    "    m = nn.ZeroPad2d((0, 1, 0, 0))\n",
    "    targets = m(targets)\n",
    "\n",
    "    return upd_batch, targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size=128):\n",
    "    data = np.array(data)\n",
    "\n",
    "    indx = np.random.permutation((len(data)))\n",
    "    n_batches = int(len(data) / batch_size) + 1\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        bound_l = batch_size*i\n",
    "        bound_r = batch_size*(i+1) if i + 1 < n_batches else len(indx)\n",
    "\n",
    "        batch_ind = indx[bound_l:bound_r]\n",
    "        batch = data[batch_ind]\n",
    "\n",
    "        yield preprocess_batch(batch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "# Buggy, misses the last sequence\n",
    "def batch_generator_max_tokens(data, max_tokens):\n",
    "    cur_batch_start = 0\n",
    "    batches = []\n",
    "    while cur_batch_start < len(data):\n",
    "        cur_batch_end, cur_batch = get_next_batch(cur_batch_start, data, max_tokens)\n",
    "        cur_batch_start = cur_batch_end + 1\n",
    "        batches.append(cur_batch)\n",
    "    return batches\n",
    "\n",
    "def get_next_batch(start, data, max_tokens):\n",
    "    tokens = 0\n",
    "    cur = start\n",
    "    while (tokens <= max_tokens) and cur < len(data):\n",
    "        s = data[cur]\n",
    "        tokens += len(s)\n",
    "        cur += 1\n",
    "    end = cur - 1\n",
    "    return end - 1, data[start:end]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1, 9, 9, 2],\n         [1, 9, 9, 2],\n         [1, 9, 9, 2],\n         [1, 9, 9, 2],\n         [1, 9, 9, 2]]),\n tensor([[9, 9, 2, 0],\n         [9, 9, 2, 0],\n         [9, 9, 2, 0],\n         [9, 9, 2, 0],\n         [9, 9, 2, 0]]))"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t, target_t = preprocess_batch(x_train[:5])\n",
    "batch_t, target_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.0250,  0.1825, -0.0726, -0.0624, -0.1122, -0.2633,  0.1887,\n          -0.1222, -0.1818, -0.0588, -0.1243, -0.2717,  0.2647, -0.0008,\n          -0.0827],\n         [ 0.0737,  0.0620, -0.1059,  0.0104,  0.0229, -0.3546,  0.2105,\n           0.0421, -0.3234, -0.0183, -0.0643, -0.2593,  0.1713, -0.0112,\n          -0.1340],\n         [ 0.1400,  0.0787, -0.1382,  0.0712,  0.0991, -0.3531,  0.1966,\n           0.0918, -0.2964,  0.0100, -0.0681, -0.2550,  0.1680, -0.0317,\n          -0.1518],\n         [ 0.1828,  0.1128, -0.0863,  0.2503,  0.2203, -0.1676,  0.2259,\n           0.0418, -0.1701,  0.0978,  0.1026, -0.0681,  0.0622,  0.0215,\n          -0.2344]],\n\n        [[ 0.0250,  0.1825, -0.0726, -0.0624, -0.1122, -0.2633,  0.1887,\n          -0.1222, -0.1818, -0.0588, -0.1243, -0.2717,  0.2647, -0.0008,\n          -0.0827],\n         [ 0.0737,  0.0620, -0.1059,  0.0104,  0.0229, -0.3546,  0.2105,\n           0.0421, -0.3234, -0.0183, -0.0643, -0.2593,  0.1713, -0.0112,\n          -0.1340],\n         [ 0.1400,  0.0787, -0.1382,  0.0712,  0.0991, -0.3531,  0.1966,\n           0.0918, -0.2964,  0.0100, -0.0681, -0.2550,  0.1680, -0.0317,\n          -0.1518],\n         [ 0.1828,  0.1128, -0.0863,  0.2503,  0.2203, -0.1676,  0.2259,\n           0.0418, -0.1701,  0.0978,  0.1026, -0.0681,  0.0622,  0.0215,\n          -0.2344]],\n\n        [[ 0.0250,  0.1825, -0.0726, -0.0624, -0.1122, -0.2633,  0.1887,\n          -0.1222, -0.1818, -0.0588, -0.1243, -0.2717,  0.2647, -0.0008,\n          -0.0827],\n         [ 0.0737,  0.0620, -0.1059,  0.0104,  0.0229, -0.3546,  0.2105,\n           0.0421, -0.3234, -0.0183, -0.0643, -0.2593,  0.1713, -0.0112,\n          -0.1340],\n         [ 0.1400,  0.0787, -0.1382,  0.0712,  0.0991, -0.3531,  0.1966,\n           0.0918, -0.2964,  0.0100, -0.0681, -0.2550,  0.1680, -0.0317,\n          -0.1518],\n         [ 0.1828,  0.1128, -0.0863,  0.2503,  0.2203, -0.1676,  0.2259,\n           0.0418, -0.1701,  0.0978,  0.1026, -0.0681,  0.0622,  0.0215,\n          -0.2344]],\n\n        [[ 0.0250,  0.1825, -0.0726, -0.0624, -0.1122, -0.2633,  0.1887,\n          -0.1222, -0.1818, -0.0588, -0.1243, -0.2717,  0.2647, -0.0008,\n          -0.0827],\n         [ 0.0737,  0.0620, -0.1059,  0.0104,  0.0229, -0.3546,  0.2105,\n           0.0421, -0.3234, -0.0183, -0.0643, -0.2593,  0.1713, -0.0112,\n          -0.1340],\n         [ 0.1400,  0.0787, -0.1382,  0.0712,  0.0991, -0.3531,  0.1966,\n           0.0918, -0.2964,  0.0100, -0.0681, -0.2550,  0.1680, -0.0317,\n          -0.1518],\n         [ 0.1828,  0.1128, -0.0863,  0.2503,  0.2203, -0.1676,  0.2259,\n           0.0418, -0.1701,  0.0978,  0.1026, -0.0681,  0.0622,  0.0215,\n          -0.2344]],\n\n        [[ 0.0250,  0.1825, -0.0726, -0.0624, -0.1122, -0.2633,  0.1887,\n          -0.1222, -0.1818, -0.0588, -0.1243, -0.2717,  0.2647, -0.0008,\n          -0.0827],\n         [ 0.0737,  0.0620, -0.1059,  0.0104,  0.0229, -0.3546,  0.2105,\n           0.0421, -0.3234, -0.0183, -0.0643, -0.2593,  0.1713, -0.0112,\n          -0.1340],\n         [ 0.1400,  0.0787, -0.1382,  0.0712,  0.0991, -0.3531,  0.1966,\n           0.0918, -0.2964,  0.0100, -0.0681, -0.2550,  0.1680, -0.0317,\n          -0.1518],\n         [ 0.1828,  0.1128, -0.0863,  0.2503,  0.2203, -0.1676,  0.2259,\n           0.0418, -0.1701,  0.0978,  0.1026, -0.0681,  0.0622,  0.0215,\n          -0.2344]]], grad_fn=<AddBackward0>)"
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(batch_t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'insert'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [209], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (x_batch, y_batch) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(batch_generator(x_train[:\u001B[38;5;241m5\u001B[39m])):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(i)\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(x_batch)\n",
      "Cell \u001B[0;32mIn [205], line 14\u001B[0m, in \u001B[0;36mbatch_generator\u001B[0;34m(data, batch_size)\u001B[0m\n\u001B[1;32m     11\u001B[0m batch_ind \u001B[38;5;241m=\u001B[39m indx[bound_l:bound_r]\n\u001B[1;32m     12\u001B[0m batch \u001B[38;5;241m=\u001B[39m data[batch_ind]\n\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m \u001B[43mpreprocess_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [204], line 7\u001B[0m, in \u001B[0;36mpreprocess_batch\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(batch):\n\u001B[1;32m      6\u001B[0m     seq \u001B[38;5;241m=\u001B[39m batch[i]\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m----> 7\u001B[0m     seq \u001B[38;5;241m=\u001B[39m \u001B[43madd_start\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     seq \u001B[38;5;241m=\u001B[39m add_end(seq)\n\u001B[1;32m      9\u001B[0m     seq \u001B[38;5;241m=\u001B[39m add_padding(seq, amt\u001B[38;5;241m=\u001B[39mmax_len \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(seq))\n",
      "Cell \u001B[0;32mIn [90], line 2\u001B[0m, in \u001B[0;36madd_start\u001B[0;34m(seq)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_start\u001B[39m(seq):\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mseq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minsert\u001B[49m(\u001B[38;5;241m0\u001B[39m, w2i[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.start\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m seq\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'insert'"
     ]
    }
   ],
   "source": [
    "for i, (x_batch, y_batch) in enumerate(batch_generator(x_train[:5])):\n",
    "    print(i)\n",
    "    print(x_batch)\n",
    "    print(y_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def sample(lnprobs, temperature=1.0):\n",
    "    \"\"\"\n",
    "     Sample an element from a categorical distribution\n",
    "     :param lnprobs: Outcome logits\n",
    "     :param temperature: Sampling temperature. 1.0 follows the given distribution, 0.0 returns the maximum probability element.\n",
    "     :return: The index of the sampled element.\n",
    "    \"\"\"\n",
    "    if temperature == 0.0:\n",
    "             return lnprobs.argmax()\n",
    "\n",
    "    p = F.softmax(lnprobs / temperature, dim=0)\n",
    "    cd = dist.Categorical(p)\n",
    "    return cd.sample()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def predict(dataset, model, seq, temperature=1.0, max_length=20):\n",
    "    \"\"\"\n",
    "    :param dataset: need i2w and w2i\n",
    "    :param model: the model we sample from\n",
    "    :param seq: the sequence of tokens we want to complete\n",
    "    :param max_length: we stop if we reach an end token, or after max_length tokens\n",
    "    :return: the generated sequence of tokens\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    for i in range(0, max_length):\n",
    "        x = torch.tensor([[dataset.w2i[i] for w in seq[i:]]])\n",
    "        y = model.forward(x)\n",
    "        last_token_logits = y[0][-1]\n",
    "        j = sample(last_token_logits, temperature)\n",
    "        pred.append(seq.dataset.i2w[j])\n",
    "        if seq.dataset.i2w[j] == '.end':\n",
    "            return pred\n",
    "    return pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# For Google Colab\n",
    "device = torch.device('cuda' if torch.has_cuda else 'cpu')\n",
    "net.to(device)\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs=3, batch_size=128, learning_rate=0.001):\n",
    "\n",
    "    # Loss function:\n",
    "    # check whether the loss function applies softmax or whether we need to do it manually\n",
    "    # loss function = cross entropy loss at every point in time, read doc to figure out\n",
    "    # how to shuffle dimensions properly\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer:\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    metrics = {\n",
    "        'loss_history': [],\n",
    "        'loss_train': []\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        start_time = time()\n",
    "        running_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(batch_generator(x_train)):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            finish_time = time()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            if i % 20 == 19:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20:.3f} time: {finish_time - start_time:.3f}')\n",
    "                metrics['loss_history'].append(running_loss / 20)\n",
    "                running_loss = 0.0\n",
    "        metrics['loss_train'].append(total_loss / len(x_train))\n",
    "\n",
    "\n",
    "        print(\"Predicting:\")\n",
    "        model.eval()\n",
    "        seq = ['.start', 'a', 'b']\n",
    "        predict(model, dataset, seq, max_length=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/cdyfs0g13j3_15mc99_0nfmh0000gn/T/ipykernel_41123/831322567.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.array(data)\n",
      "/var/folders/pp/cdyfs0g13j3_15mc99_0nfmh0000gn/T/ipykernel_41123/532775599.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(upd_batch, dtype=torch.long)[:, 1:]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [128, 15], got [128, 56]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [107], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m metrics \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [106], line 30\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, dataset, epochs, batch_size, learning_rate)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Compute prediction error\u001B[39;00m\n\u001B[1;32m     29\u001B[0m pred \u001B[38;5;241m=\u001B[39m model(X)\n\u001B[0;32m---> 30\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[1;32m     33\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DeepLearning_VU/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DeepLearning_VU/lib/python3.10/site-packages/torch/nn/modules/loss.py:1164\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1165\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1166\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DeepLearning_VU/lib/python3.10/site-packages/torch/nn/functional.py:3014\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   3012\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3013\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3014\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected target size [128, 15], got [128, 56]"
     ]
    }
   ],
   "source": [
    "metrics = train(net, x_train, epochs=5, batch_size=128, learning_rate=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}