{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from data_rnn import load_ndfa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n=150000\n",
    "x_train, (i2w, w2i) = load_ndfa(n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Dictionary:{w2i}')\n",
    "print(f'Index:{i2w}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_sequence(x_train, i):\n",
    "    print(f\"Sequence #{str(i).rjust(6, ' ')}: {''.join([i2w[i] for i in x_train[i]])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in np.random.randint(n, size=10):\n",
    "    print_sequence(x_train, i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train[74191]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def batch_length(batch):\n",
    "    return max(len(seq) for seq in batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_padding(seq, amt=1):\n",
    "    for _ in range(amt):\n",
    "        seq.append(w2i['.pad'])\n",
    "    return seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_start(seq):\n",
    "    seq.insert(0, w2i['.start'])\n",
    "    return seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_end(seq):\n",
    "    seq.append(w2i['.end'])\n",
    "    return seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_batch(batch):\n",
    "    max_len = batch_length(batch)\n",
    "\n",
    "    upd_batch = []\n",
    "    for i, _ in enumerate(batch):\n",
    "        seq = batch[i].copy()\n",
    "        seq = add_start(seq)\n",
    "        seq = add_end(seq)\n",
    "        seq = add_padding(seq, amt=max_len + 2 - len(seq))\n",
    "        upd_batch.append(seq)\n",
    "\n",
    "    upd_batch = torch.tensor(upd_batch, dtype=torch.long)\n",
    "    targets = torch.tensor(upd_batch, dtype=torch.long)[:, 1:]\n",
    "    m = nn.ZeroPad2d((0, 1, 0, 0))\n",
    "    targets = m(z)\n",
    "\n",
    "    return upd_batch, targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def batch_generator(data, max_number_of_tokens=128):\n",
    "#     total_tokens = 0\n",
    "#     for i, seq in enumerate(data):\n",
    "#         batch_start = i\n",
    "#         if (total_tokens + len(data) < max_number_of_tokens):\n",
    "#             total_tokens += len(seq)\n",
    "#             batch_end = i + len(data)\n",
    "#         batch = data[batch_start:batch_end]\n",
    "#         yield preprocess_batch(batch)\n",
    "#         yield(batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size=128):\n",
    "    data = np.array(data)\n",
    "\n",
    "    indx = np.random.permutation((len(data)))\n",
    "    n_batches = int(len(data) / batch_size) + 1\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        bound_l = batch_size*i\n",
    "        bound_r = batch_size*(i+1) if i + 1 < n_batches else len(indx)\n",
    "\n",
    "        batch_ind = indx[bound_l:bound_r]\n",
    "        batch = data[batch_ind]\n",
    "\n",
    "        yield preprocess_batch(batch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_t, target_t = preprocess_batch(x_train[:5])\n",
    "batch_t, target_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, (x_batch, y_batch) in enumerate(batch_generator(x_train[:5])):\n",
    "    print(i)\n",
    "    print(x_batch)\n",
    "    print(y_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_size=len(w2i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "hidden_size = 16\n",
    "lstm_num_layers = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 hidden_size,\n",
    "                 lstm_num_layers) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, lstm_num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            input = self.embedding(x)\n",
    "            lstm_output, (hn, cn) = self.lstm(input)\n",
    "            output = self.linear(lstm_output)\n",
    "            return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net =  Net(vocab_size, embedding_size, hidden_size, lstm_num_layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample(lnprobs, temperature=1.0):\n",
    "    \"\"\"\n",
    "     Sample an element from a categorical distribution\n",
    "     :param lnprobs: Outcome logits\n",
    "     :param temperature: Sampling temperature. 1.0 follows the given distribution, 0.0 returns the maximum probability element.\n",
    "     :return: The index of the sampled element.\n",
    "    \"\"\"\n",
    "    if temperature == 0.0:\n",
    "             return lnprobs.argmax()\n",
    "\n",
    "    p = F.softmax(lnprobs / temperature, dim=0)\n",
    "    cd = dist.Categorical(p)\n",
    "    return cd.sample()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(dataset, model, seq, temperature=1.0, max_length=20):\n",
    "    \"\"\"\n",
    "    :param dataset: need i2w and w2i\n",
    "    :param model: the model we sample from\n",
    "    :param seq: the sequence of tokens we want to complete\n",
    "    :param max_length: we stop if we reach an end token, or after max_length tokens\n",
    "    :return: the generated sequence of tokens\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    for i in range(0, max_length):\n",
    "        x = torch.tensor([[dataset.w2i[i] for w in seq[i:]]])\n",
    "        y = model.forward(x)\n",
    "        last_token_logits = y[0][-1]\n",
    "        j = sample(last_token_logits, temperature)\n",
    "        pred.append(seq.dataset.i2w[j])\n",
    "        if seq.dataset.i2w[j] == '.end':\n",
    "            return pred\n",
    "    return pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.has_mps else 'cpu')\n",
    "net.to(device)\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs=3, batch_size=128, learning_rate=0.001):\n",
    "\n",
    "    # Loss function:\n",
    "    # check whether the loss function applies softmax or whether we need to do it manually\n",
    "    # loss function = cross entropy loss at every point in time, read doc to figure out\n",
    "    # how to shuffle dimensions properly\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer:\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    metrics = {\n",
    "        'loss_history': [],\n",
    "        'loss_train': []\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "        start_time = time()\n",
    "        running_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "\n",
    "        size = len(dataloader.dataset)\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(batch_generator(x_train)):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            finish_time = time()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            if i % 20 == 19:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20:.3f} time: {finish_time - start_time:.3f}')\n",
    "                metrics['loss_history'].append(running_loss / 20)\n",
    "                running_loss = 0.0\n",
    "        metrics['loss_train'].append(total_loss / len(x_train))\n",
    "\n",
    "\n",
    "        print(\"Predicting:\")\n",
    "        model.eval()\n",
    "        seq = ['.start', 'a', 'b']\n",
    "        predict(model, dataset, seq, max_length=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}