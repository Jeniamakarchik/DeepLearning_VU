{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e38d3667ff54170a10908e74cf22e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dca77e5bd284b509cbc4fd1f7c944e1",
              "IPY_MODEL_7958830441544c4aab5236996ce3f52a",
              "IPY_MODEL_dfecafec186e40bab904eb619ea107ca"
            ],
            "layout": "IPY_MODEL_67f945ec7580469b95ef6463e5563b11"
          }
        },
        "5dca77e5bd284b509cbc4fd1f7c944e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da19d8d9336a49e6a1720f04ddaa4aee",
            "placeholder": "​",
            "style": "IPY_MODEL_3ebfb5764c524bbeba9394088f06bc7b",
            "value": ""
          }
        },
        "7958830441544c4aab5236996ce3f52a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b951b7319e3543f8bba6bc26b3b19de8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e761c7b631640a89a26e7155aeac3c6",
            "value": 1
          }
        },
        "dfecafec186e40bab904eb619ea107ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df5e4137419407fa4c80804670a4211",
            "placeholder": "​",
            "style": "IPY_MODEL_3a3284d7f5fb4566bc6920e56f16d59d",
            "value": " 890/? [00:05&lt;00:00, 116.28it/s]"
          }
        },
        "67f945ec7580469b95ef6463e5563b11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da19d8d9336a49e6a1720f04ddaa4aee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ebfb5764c524bbeba9394088f06bc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b951b7319e3543f8bba6bc26b3b19de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1e761c7b631640a89a26e7155aeac3c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7df5e4137419407fa4c80804670a4211": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a3284d7f5fb4566bc6920e56f16d59d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f9e9ae48abc41639ccc31eb7d6ddda8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af75d1a4b05d43aa827e3590e1e75339",
              "IPY_MODEL_2a3aa766da55436fa140963e95ec956c",
              "IPY_MODEL_a390c2dba18a4ad2bca2548bd07fc5dc"
            ],
            "layout": "IPY_MODEL_a7bc795862324942bf869290ce138852"
          }
        },
        "af75d1a4b05d43aa827e3590e1e75339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac4be491f8fe41c28aa2fd17d58f7440",
            "placeholder": "​",
            "style": "IPY_MODEL_50fc3dd7ac5d40a0896d5df51ec853c5",
            "value": ""
          }
        },
        "2a3aa766da55436fa140963e95ec956c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_914f3f011aa94a339d84c593ebd6cff3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f8cb4ba720d4d99ab0522e213c2d3cb",
            "value": 1
          }
        },
        "a390c2dba18a4ad2bca2548bd07fc5dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaca3fd654d84a3aa8eaf0f51d34dda8",
            "placeholder": "​",
            "style": "IPY_MODEL_607e0a818b8d4739a2835f6cef6c8360",
            "value": " 111/? [00:00&lt;00:00, 119.83it/s]"
          }
        },
        "a7bc795862324942bf869290ce138852": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac4be491f8fe41c28aa2fd17d58f7440": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50fc3dd7ac5d40a0896d5df51ec853c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "914f3f011aa94a339d84c593ebd6cff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9f8cb4ba720d4d99ab0522e213c2d3cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaca3fd654d84a3aa8eaf0f51d34dda8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "607e0a818b8d4739a2835f6cef6c8360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd6ffc9b8aff4df0b3ef4ccb68ef5119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_972073bb67234d9c9416874984241cd4",
              "IPY_MODEL_1193e220c5084ecaad7070a60f64791e",
              "IPY_MODEL_393506cb51484cdebe4672c6616ffe36"
            ],
            "layout": "IPY_MODEL_b2b144d6744f4e4c800bb9392fde4f35"
          }
        },
        "972073bb67234d9c9416874984241cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4615605b09f4218ae4eabce46cd0f84",
            "placeholder": "​",
            "style": "IPY_MODEL_a77a8b0b79014ebfbda20179f7bf08bf",
            "value": ""
          }
        },
        "1193e220c5084ecaad7070a60f64791e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_970470e333324fb486c8d6443f615416",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20e1d12eda9b4ee3ad4ebb2f99a29d80",
            "value": 1
          }
        },
        "393506cb51484cdebe4672c6616ffe36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1093d191dae54bd5b27521c71eb67a42",
            "placeholder": "​",
            "style": "IPY_MODEL_a5462f3e4fc34d6daef49901012dd41c",
            "value": " 112/? [00:00&lt;00:00, 139.90it/s]"
          }
        },
        "b2b144d6744f4e4c800bb9392fde4f35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4615605b09f4218ae4eabce46cd0f84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a77a8b0b79014ebfbda20179f7bf08bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "970470e333324fb486c8d6443f615416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "20e1d12eda9b4ee3ad4ebb2f99a29d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1093d191dae54bd5b27521c71eb67a42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5462f3e4fc34d6daef49901012dd41c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ_pmgxvGur9"
      },
      "source": [
        "# Assignment 4b - Graph Convolutional Networks\n",
        "## Deep Learning Course - Vrije Universiteit Amsterdam, 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Students:\n",
        "- Adwitiya Mandal - aml231\n",
        "- Florence Berbain - fbe222\n",
        "- Yauheniya Makarevich - ema269"
      ],
      "metadata": {
        "id": "XtjoVLLFfqRJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEneMITS2agU"
      },
      "source": [
        "#### Instructions on how to use this notebook:\n",
        "\n",
        "This notebook is hosted on Google Colab. To be able to work on it, you have to create your own copy. Go to *File* and select *Save a copy in Drive*.\n",
        "\n",
        "You can also avoid using Colab entirely, and download the notebook to run it on your own machine. If you choose this, go to *File* and select *Download .ipynb*.\n",
        "\n",
        "The advantage of using Colab is that you can use a GPU. You can complete this assignment with a CPU, but it will take a bit longer. Furthermore, we encourage you to train using the GPU not only for faster training, but also to get experience with this setting. This includes moving models and tensors to the GPU and back. This experience is very valuable because for many interesting models and large datasets (like large CNNs for ImageNet, or Transformer models trained on Wikipedia), training on GPU is the only feasible way.\n",
        "\n",
        "The default Colab runtime does not have a GPU. To change this, go to *Runtime - Change runtime type*, and select *GPU* as the hardware accelerator. The GPU that you get changes according to what resources are available at the time, and its memory can go from a 5GB, to around 18GB if you are lucky. If you are curious, you can run the following in a code cell to check:\n",
        "\n",
        "```sh\n",
        "!nvidia-smi\n",
        "```\n",
        "\n",
        "Note that despite the name, Google Colab does  not support collaborative work without issues. When two or more people edit the notebook concurrently, only one version will be saved. You can choose to do group programming with one person sharing the screen with the others, or make multiple copies of the notebook to work concurrently.\n",
        "\n",
        "**Submission:** Upload your notebook in .ipynb format to Canvas. The code and answers to the questions in the notebook are sufficient, no separate report is expected. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBgoJIpdLI2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224b3a1f-4bfb-4d0f-86be-91c3ceb30dbf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 23 19:24:07 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0    28W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsdc7fDp40rQ"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Graphs are very useful data structures that allow us to represent sets of entities and the way they are related among each other. In a graph, entities are also known as *nodes*, and any link between entities is also called an *edge*.\n",
        "\n",
        "Examples of real world objects that can be modeled as graphs are social networks, where entities are people and relations denote friendship; and molecules, where entities are atoms and relations indicate a bond between them.\n",
        "\n",
        "There has been increased interest in the recent years in the application of deep learning architectures to graph-structured data, for tasks like predicting missing relations between entities, classifying entities, and classifying graphs. This interest has been spurred by the introduction of Graph Convolutional Networks (GCNs).\n",
        "\n",
        "In this assignment, you will implement and experiment with one of the first versions of the GCN, proposed by Thomas Kipf and Max Welling in their 2017 paper, [Semi-supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907). In particular, the goals of this assignment are to\n",
        "\n",
        "- Understand how GCNs are formulated\n",
        "- Implement the GCN using PyTorch\n",
        "- Train and evaluate a model for semi-supervised node classification in citation networks\n",
        "- Train and evaluate a model for binary classification of molecules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvsuVNczG6pP"
      },
      "source": [
        "### Representing graphs\n",
        "\n",
        "Suppose we have the following graph:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/01-graph.png\" width=\"200\">\n",
        "\n",
        "This is an undirected graph (since the edges have no specified direction) with 4 nodes. One way to represent the connectivity structure of the graph is by means of the **adjacency matrix**. The $i$-th row of the matrix contains a 1 in the $j$-th column, if nodes $i$ and $j$ are connected. For an undirected graph like the one above, this means that the adjacency matrix\n",
        "\n",
        "- Is symmetric (e.g. an edge between 0 and 2 is equivalent as an edge between 2 and 0)\n",
        "- Is square, of size $n\\times n$ where $n$ is the number of nodes\n",
        "\n",
        "The adjacency matrix for the graph above is then the following:\n",
        "\n",
        "$$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "0 & 0 & 1 & 0 \\\\ \n",
        "0 & 0 & 1 & 0 \\\\\n",
        "1 & 1 & 0 & 1 \\\\\n",
        "0 & 0 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "A second matrix of interest is the **degree matrix**. This is a diagonal matrix where the $i$-th element of the diagonal indicates the number of edges connected to node $i$. Note that these can be obtained from $A$ by summing across the columns, or the rows. For our example, the degree matrix is\n",
        "\n",
        "$$\n",
        "D = \\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 \\\\ \n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 3 & 0 \\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "For specific applications, each node in the graph will have an associated vector of features $x\\in\\mathbb{R}^c$. If our graph is a social network, then the vector of features can contain information like age, location, and musical tastes, in a specific numeric format. In the case of a molecule, the node could represent an atom and have features like the atomic mass, etc. We can lay out the features in a matrix $X\\in\\mathbb{R}^{n\\times c}$, so that the feature vector for node $i$ is in the $i$-th row."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCEQ2ffzHCf2"
      },
      "source": [
        "### Loading a citation network\n",
        "\n",
        "To move to a real world example, we will start with the Cora dataset. This dataset represents a citation network, where nodes are scientific publications, edges denote citations between them, and features are a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) extracted from their contents.\n",
        "\n",
        "This graph contains labels for nodes, that represent a specific topic. We will use these for a node classification task.\n",
        "\n",
        "To easily load it, we will use [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html) (PyG), a deep learning library for graph-structured data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd2bTEBADt-a"
      },
      "source": [
        "# Install PyTorch Geometric\n",
        "import torch\n",
        "try:\n",
        "    import torch_geometric\n",
        "except ModuleNotFoundError:\n",
        "    # Installing torch geometric packages with specific CUDA+PyTorch version.\n",
        "    # See https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for details\n",
        "    TORCH = torch.__version__.split('+')[0]\n",
        "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "\n",
        "    !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-geometric\n",
        "    import torch_geometric"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK-j-jb-RER_",
        "outputId": "4378886d-b0ce-4031-9446-a27779c93812"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.8/dist-packages (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Nvh_-qEo1q"
      },
      "source": [
        "We can now use the library to download and import the dataset. Initializing the `Planetoid` class returns a `Dataset` object that can contain multiple graphs. In this task we will only use the `Cora` dataset (the citation network) and hence, we will select only the first element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuOvwhsHD2YK"
      },
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "data = Planetoid(root='data/Planetoid', name='Cora')[0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4WZkoiHFyZm"
      },
      "source": [
        "\n",
        "#### Question 1 (0.25 pt)\n",
        "\n",
        "The `data` object is an instance of the `Data` class in PyG. Check the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html) and report the following properties of the graph:\n",
        "\n",
        "- Number of nodes\n",
        "- Number of edges \n",
        "- The dimension $c$ of the feature vectors $x\\in\\mathbb{R}^c$\n",
        "- The number of targets for the classification task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjVuGJhlJC_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f54a213e-8383-4fdf-eeb6-83fddb551bb9"
      },
      "source": [
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Feature dimension: {data.num_node_features}')\n",
        "print(f'Number of targets: {len(torch.unique(data.y))}')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 2708\n",
            "Number of edges: 10556\n",
            "Feature dimension: 1433\n",
            "Number of targets: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4DrGDAuJ2YO"
      },
      "source": [
        "#### Question 2 (0.25 pt)\n",
        "\n",
        "In PyG, edges are provided in a tensor of shape (2, number of edges). You can access it via `data.edge_index`. Each column in this tensor contains the IDs for two nodes that are connected in the graph.\n",
        "\n",
        "We saw that in an undirected graph, an edge between nodes $i$ and $j$ adds a value of 1 to positions $(i, j)$ and $(j, i)$ of the adjacency matrix. Is this also true for the edge index? That is, if there is an edge $(i, j)$ in `data.edge_index`, is there also an edge for $(j, i)$? This is important to know for the next steps of the implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTRfNxibarRZ"
      },
      "source": [
        "def check_edges(data):\n",
        "  edge_dict = {}\n",
        "\n",
        "  if data.is_directed():\n",
        "    print('Graph is not directed.')\n",
        "    return False\n",
        "\n",
        "  sum_ = torch.sum(data.edge_index, axis = 1)\n",
        "  if sum_[0].item() != sum_[1].item():\n",
        "    print('Amount of out nodes does not correspond amount of in nodes.')\n",
        "    return False\n",
        "\n",
        "  for i, j in zip(data.edge_index[0], data.edge_index[1]):\n",
        "    if i.item() not in edge_dict.keys():\n",
        "      edge_dict[i.item()] = []\n",
        "    \n",
        "    edge_dict[i.item()].append(j.item())\n",
        "\n",
        "  for key in edge_dict.keys():\n",
        "    edges = edge_dict[key]\n",
        "    for e in edges:\n",
        "      if key not in edge_dict[e]:\n",
        "        print(f'There is no ({key}, {e}) edge!')\n",
        "        return False\n",
        "  \n",
        "  return True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_edges(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4wxXZeQRjat",
        "outputId": "890ea9c1-8629-4cb6-f2db-bf67598bcf2a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOpS3QTYiOqp"
      },
      "source": [
        "#### Question 3 (0.5 pt)\n",
        "\n",
        "In graphs, especially large ones, the adjacency matrix is **sparse**: most entries are zero. Sparse matrices allow for efficient storage and computation.\n",
        "\n",
        "To prepare and pre-process sparse matrices, we will use [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html). Once the matrices are ready, we will convert them to PyTorch tensors.\n",
        "\n",
        "We will use the [Sparse COO format](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)). We encourage you to first get familiar with how it works after continuing with the assignment.\n",
        "\n",
        "- Use the [`scipy.sparse.coo_matrix()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html) function to build the adjacency matrix. Think of what arguments are needed, and how you can obtain them from the graph data loaded above.\n",
        "- Use the `sum()` method of sparse matrices, together with `scipy.sparse.diags()`, to compute the degree matrix using the definition above.\n",
        "\n",
        "Both resulting matrices must be sparse of type `float32`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix, diags"
      ],
      "metadata": {
        "id": "Bfp4Mbv8R_kZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjacency matrix"
      ],
      "metadata": {
        "id": "gkkfGYSfSbmT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC01OjbJs92-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5d686d0-c8ae-4c9e-89bd-61b8c8acb83e"
      },
      "source": [
        "rows = np.array(data.edge_index[0])\n",
        "cols = np.array(data.edge_index[1])\n",
        "d = np.ones(cols.shape)\n",
        "\n",
        "adj_matrix = coo_matrix((d, (rows, cols)), shape=data.size())\n",
        "adj_matrix.toarray()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adj_matrix.toarray()[0, 633]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1wrilx2SJ8-",
        "outputId": "89e095bc-5868-466f-d7bc-517a9136f57f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adj_matrix.toarray()[633, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfOh_FQaSLVg",
        "outputId": "5753056c-b5e4-4b37-e137-53a5a7676f69"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert sum(sum(adj_matrix.toarray())) == len(data.edge_index[0])"
      ],
      "metadata": {
        "id": "EOYfAYfBSM-L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Degree matrix"
      ],
      "metadata": {
        "id": "3t1vRXi7Sdjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deg_matrix = diags(sum(adj_matrix.toarray()))\n",
        "deg_matrix.toarray()[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGj-Dy4lSXcM",
        "outputId": "9634dbca-5a59-494d-be36-7eb1bb95fee8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIEJyQi2TzyY"
      },
      "source": [
        "You might wonder why we suggest to use a scipy sparse matrix, while also PyTorch supports them. The reason is that in the next step, we will be multiplying two sparse matrices, an operation not supported in PyTorch. PyTorch only allows multiplying a sparse matrix with a dense one, something which we will be doing at a later stage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlmzSb0up4LB"
      },
      "source": [
        "### The Graph Convolutional Network\n",
        "\n",
        "The goal of the graph convolution is to take the feature vectors of all nodes $X\\in\\mathbb{R}^{n\\times c}$, and propagate them along the existing edges, to obtain updated representations $Z\\in\\mathbb{R}^{n\\times d}$.\n",
        "\n",
        "\n",
        "The GCN is initially motivated as performing a convolution, similarly as it is done in CNNs for images, for graph-structured data. In Kipf and Welling (2017), a theoretical derivation leads to the following formula:\n",
        "\n",
        "$$\n",
        "Z = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}XW\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- $W\\in\\mathbb{R}^{c\\times d}$ is a matrix of parameters to be learned via gradient descent\n",
        "- $\\tilde{A} = A + I_n$, where $I_n$ is an $n\\times n$ identity matrix\n",
        "- $\\tilde{D}$ is the degree matrix computed with $\\tilde{A}$ as the adjacency matrix\n",
        "\n",
        "If we define $\\hat{A} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$, the graph convolution can be written as $Z = \\hat{A}XW$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL4b-MTvysBp"
      },
      "source": [
        "#### Question 4 (0.25 pt)\n",
        "\n",
        "Given the formula for the GCN, explain why it operates by propagating feature vectors across the graph. To answer this, it might be useful to recall the definitions of the adjacency and degree matrices, and how they are involved in the formula."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgx2SkTTyiSN"
      },
      "source": [
        "- Adjacency matrix - matrix of connections (including self-connection)\n",
        "- Degree matrix - diag matrix with the amount of nodes connected to the node (including self-connection)\n",
        "\n",
        "Approach with calculating adjacency matrix allows us to aggregate information not only from our current node (for this reason we added identity matrix to A), but also from the neighbours. Matrices D^(-1/2) are used as a per-neighbour normalization to adjacency matrix to have weighted sum of the value of the current node and its neighbours. These computations simulate the behavior of concolution layer - gathering info from neighbours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUGABEqxylsd"
      },
      "source": [
        "#### Question 5 (0.5 pt)\n",
        "\n",
        "Compute the **normalized adjacency matrix** $\\hat{A}$. The result should be a sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.linalg import fractional_matrix_power"
      ],
      "metadata": {
        "id": "lZABGNg3S2uI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPZbnSaSyDzO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38dad5cd-5eec-4666-8137-debee2f4bfad"
      },
      "source": [
        "I = np.eye(adj_matrix.shape[0])\n",
        "A_ = adj_matrix.toarray() + I\n",
        "print(sum(A_))\n",
        "D_ = diags(sum(A_), dtype=np.float32)\n",
        "D_inv = fractional_matrix_power(D_.toarray(), -1/2)\n",
        "A_norm = np.matmul(np.matmul(D_inv, A_), D_inv)\n",
        "A_norm = coo_matrix(A_norm)\n",
        "A_norm"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4. 4. 6. ... 2. 5. 5.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2708x2708 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 13264 stored elements in COOrdinate format>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A_norm.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7zCTZhGU4dv",
        "outputId": "7bec577c-aa31-4e82-afa0-747aada0914f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.25      , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.25      , 0.20412415, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.20412415, 0.16666668, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.49999998, 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.20000002,\n",
              "        0.20000002],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.20000002,\n",
              "        0.20000002]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLLdGdZoMEy-"
      },
      "source": [
        "#### Question 6 (0.5 pt)\n",
        "\n",
        "So far we have used scipy to build and compute sparse matrices. Since we want to train a GCN with PyTorch, we need to convert $\\hat{A}$ into a sparse PyTorch tensor. You can do this with the [`torch.sparse_coo_tensor()`](https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html) function, making sure to specify `torch.float` as the type."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(A_norm.row)\n",
        "print(A_norm.col)\n",
        "print(A_norm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQtmUzIrWFVt",
        "outputId": "184eeb48-8053-4ff6-b8e3-c8b05c5d7f81"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0    0    0 ... 2707 2707 2707]\n",
            "[   0  633 1862 ... 1473 2706 2707]\n",
            "(2708, 2708)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgDsVHzEM32F"
      },
      "source": [
        "indices = np.array((A_norm.row, A_norm.col))\n",
        "A_norm_tensor = torch.sparse_coo_tensor(indices=indices, values=A_norm.data, size=A_norm.shape)\n",
        "A_norm_tensor = A_norm_tensor.float()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_norm_tensor.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df7F6HdnWPnB",
        "outputId": "a8e4c295-0fda-4366-c2e2-9b2667390827"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2708, 2708])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAlRVT5aODkX"
      },
      "source": [
        "#### Question 7 (0.5 pt)\n",
        "\n",
        "We now have all the ingredients to build a GCN layer. Implement a class (inheriting from `torch.nn.Module`) with a learnable matrix of weights $W\\in\\mathbb{R}^{c\\times d}$. Make sure to\n",
        "\n",
        "- Call this class `GCNLayer`\n",
        "- The `__init__()` constructor should take as argument the number of input and output features.\n",
        "- Use `torch.nn.init.kaiming_uniform_` to initialize $W$.\n",
        "- Define the `forward` method, which takes as input $X$ and $\\hat{A}$ and returns $Z$. Note that multiplications involving the sparse matrix $\\hat{A}$ have to be done with `torch.spmm`. \n",
        "\n",
        "Once you have implemented the class, instantiate a layer with the correct number of input features for the Cora dataset, and a number of output features of your choice. Do a forward pass and report the shape of the output tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFCohhhwPpTT"
      },
      "source": [
        "from torch import nn"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNLayer(nn.Module):\n",
        "  def __init__(self, input_size, output_size) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.W = nn.Parameter(nn.init.kaiming_uniform_(\n",
        "        torch.empty(self.input_size, self.output_size, dtype = torch.float32, requires_grad=True), \n",
        "      ), requires_grad=True\n",
        "    )\n",
        "\n",
        "  def forward(self, x, A_norm):\n",
        "    x = torch.sparse.mm(A_norm, x)\n",
        "    output = torch.matmul(x, self.W)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "GLpeQAn0WTvg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = GCNLayer(data.x.shape[1], len(torch.unique(data.y))) # [2708, 1433] -> [2708, 7]"
      ],
      "metadata": {
        "id": "_YfDCZZ0Wjrz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = layer(data.x, A_norm_tensor)\n",
        "print(f'Output tensor shape: {output.size()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdj4hEoaWmF5",
        "outputId": "995d26f0-506e-4016-9609-877cbd99df33"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output tensor shape: torch.Size([2708, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_aPLaHKWnN0",
        "outputId": "bcd9560b-5d06-463c-c71f-948ac39b5d06"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.6324,  0.7055, -0.6272, -0.7837,  0.2708,  0.8501,  0.9186],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ptAiizZUKaM"
      },
      "source": [
        "#### Question 8 (0.5 pt)\n",
        "\n",
        "As we have seen so far, the GCN layer implements a special type of linear transformation of the inputs. However, it is often beneficial in deep learning to stack multiple, non-linear transformations of the input features. Implement a second module class for a model with two GCN layers (use the module you implemented in the previous question).\n",
        "\n",
        "- Call this class `GCN`\n",
        "- The constructor must now take as input the number of input features, the output dimension of the first layer (this is the hidden layer), and the output dimension of the output layer.\n",
        "- In the forward pass, add a ReLU activation function after the first layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zhyu3S9Vj3b"
      },
      "source": [
        "class GCN(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.gcn1 = GCNLayer(self.input_size, self.hidden_size)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.gcn2 = GCNLayer(self.hidden_size, self.output_size)\n",
        "  \n",
        "  def forward(self, x, A_norm):\n",
        "    x = self.gcn1(x, A_norm)\n",
        "    x = self.relu(x)\n",
        "    x = self.gcn2(x, A_norm)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NVB-3I5Wfkf"
      },
      "source": [
        "### GCNs for semi-supervised node classification\n",
        "\n",
        "Now that we have a GCN with two layers, we can test its performance in a node classification task. We will pass the input node features $X$ through the GCN layers, and the output will be of size $n\\times k$ where $k$ is the number of classes (which you found in question 1). The label denotes the topic an article in the citation network belongs to (e.g. physics, computer science, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trc4dSa7cuQj"
      },
      "source": [
        "#### Question 9 (1.5 pt)\n",
        "\n",
        "Note that the `data` object contains all labels (for all splits) in `data.y`, and binary masks for the train, validation, and test splits in `data.train_mask`, `data.val_mask`, and `data.test_mask`, respectively. These masks are the same size as `data.y`, and indicate which labels belong to which split.\n",
        "\n",
        "- Create a GCN with two layers (using the class from the previous question), with 32 as the hidden dimension, and the number of output features equal to the number of classes in the Cora dataset.\n",
        "\n",
        "- Use the Adam optimizer with a learning rate of 0.01.\n",
        "\n",
        "- Implement a training loop for the GCN. At each step, pass $X$ and $\\hat{A}$ to the GCN to obtain the logits. Compute the mean cross-entropy loss **only for the training instances**, using the binary masks.\n",
        "\n",
        "- After each training step, evaluate the accuracy for the validation instances.\n",
        "\n",
        "- Train for 100 epochs. Once training is finished, plot the training loss and validation accuracy (in a graph in function of the epoch number), and report the accuracy in the test set.\n",
        "\n",
        "You should obtain an accuracy over 75% on both the validation and test sets. You can also compare your results with the original paper, which also contains results for the Cora dataset. Give a brief discussion on the results of your experiments.\n",
        "\n",
        "Note that in contrast with other tasks, like image classification on some datasets, we don't use mini-batches here. The whole matrix of features and the adjacency is passed to the GCN in one step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z2OP_ZRWlmo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.classification import MulticlassAccuracy"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = data.x.shape[1]\n",
        "hidden_size = 32\n",
        "num_classes = len(torch.unique(data.y))\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "zsQrG6CIWzjW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, X, y, train_mask, val_mask, A_norm, num_classes,\n",
        "          learning_rate=0.01, epochs=100):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  accuracy = MulticlassAccuracy(num_classes=num_classes)\n",
        "\n",
        "  def calculate_accuracy(output, y_true):\n",
        "    y_prob = F.softmax(output, dim=-1)\n",
        "    acc = accuracy(y_prob, y_true)\n",
        "    return acc\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  metrics = {\n",
        "    'total_loss': [],\n",
        "    'val_acc': [],\n",
        "    'train_acc': []\n",
        "  }\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    start_time = time()\n",
        "    outputs = model(X, A_norm)\n",
        "    loss = criterion(outputs[train_mask], y[train_mask])\n",
        "    loss.backward()\n",
        "    finish_time = time()\n",
        "\n",
        "    metrics['total_loss'].append(loss.item())\n",
        "    metrics['train_acc'].append(calculate_accuracy(outputs[train_mask], y[train_mask]).cpu())\n",
        "    metrics['val_acc'].append(calculate_accuracy(outputs[val_mask], y[val_mask]).cpu())\n",
        "\n",
        "    print(f'''[{epoch + 1}] loss: {loss.item() / sum(train_mask):.6f}\n",
        "      train_acc: {metrics['train_acc'][-1]:.4f} val_acc:{metrics['val_acc'][-1]:4f} \n",
        "      time: {finish_time - start_time:.3f}\\n''')\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  return model, metrics"
      ],
      "metadata": {
        "id": "6ZZ6s_i0W1XP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(input_size, hidden_size, num_classes)"
      ],
      "metadata": {
        "id": "lHVr2sG_XELX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train ration:{sum(data.train_mask) / len(data.y):.4f}')\n",
        "print(f'Valid ration:{sum(data.val_mask) / len(data.y):.4f}')\n",
        "print(f'Test ration:{sum(data.test_mask) / len(data.y):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpG2R02CXIQe",
        "outputId": "b94c844e-d0a2-4ebb-e962-91e44a2afb49"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train ration:0.0517\n",
            "Valid ration:0.1846\n",
            "Test ration:0.3693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, metrics = train(\n",
        "    model, data.x, data.y, data.train_mask, data.val_mask, \n",
        "    A_norm_tensor, num_classes, learning_rate, epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTxngM0sXZW_",
        "outputId": "c1d22352-ead3-43ff-e3b2-c415813bf7b3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "[1] loss: 0.022250\n",
            "      train_acc: 0.0929 val_acc:0.099805 \n",
            "      time: 0.036\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "[2] loss: 0.016536\n",
            "      train_acc: 0.1714 val_acc:0.153006 \n",
            "      time: 0.028\n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "[3] loss: 0.013167\n",
            "      train_acc: 0.3143 val_acc:0.245371 \n",
            "      time: 0.031\n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "[4] loss: 0.010931\n",
            "      train_acc: 0.5000 val_acc:0.289058 \n",
            "      time: 0.047\n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "[5] loss: 0.009194\n",
            "      train_acc: 0.6500 val_acc:0.377905 \n",
            "      time: 0.038\n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "[6] loss: 0.007713\n",
            "      train_acc: 0.7214 val_acc:0.429489 \n",
            "      time: 0.031\n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "[7] loss: 0.006397\n",
            "      train_acc: 0.8000 val_acc:0.475803 \n",
            "      time: 0.030\n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "[8] loss: 0.005241\n",
            "      train_acc: 0.8571 val_acc:0.517601 \n",
            "      time: 0.034\n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "[9] loss: 0.004272\n",
            "      train_acc: 0.8929 val_acc:0.559048 \n",
            "      time: 0.028\n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "[10] loss: 0.003487\n",
            "      train_acc: 0.9429 val_acc:0.604794 \n",
            "      time: 0.028\n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "[11] loss: 0.002848\n",
            "      train_acc: 0.9500 val_acc:0.626999 \n",
            "      time: 0.027\n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "[12] loss: 0.002319\n",
            "      train_acc: 0.9714 val_acc:0.655897 \n",
            "      time: 0.035\n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "[13] loss: 0.001880\n",
            "      train_acc: 0.9786 val_acc:0.675872 \n",
            "      time: 0.026\n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "[14] loss: 0.001516\n",
            "      train_acc: 0.9857 val_acc:0.700285 \n",
            "      time: 0.029\n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "[15] loss: 0.001217\n",
            "      train_acc: 0.9929 val_acc:0.710594 \n",
            "      time: 0.030\n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "[16] loss: 0.000978\n",
            "      train_acc: 0.9929 val_acc:0.715705 \n",
            "      time: 0.038\n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "[17] loss: 0.000787\n",
            "      train_acc: 1.0000 val_acc:0.717885 \n",
            "      time: 0.027\n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "[18] loss: 0.000637\n",
            "      train_acc: 1.0000 val_acc:0.729358 \n",
            "      time: 0.027\n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "[19] loss: 0.000519\n",
            "      train_acc: 1.0000 val_acc:0.727959 \n",
            "      time: 0.026\n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "[20] loss: 0.000424\n",
            "      train_acc: 1.0000 val_acc:0.729329 \n",
            "      time: 0.032\n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "[21] loss: 0.000347\n",
            "      train_acc: 1.0000 val_acc:0.738838 \n",
            "      time: 0.029\n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "[22] loss: 0.000285\n",
            "      train_acc: 1.0000 val_acc:0.739742 \n",
            "      time: 0.030\n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "[23] loss: 0.000234\n",
            "      train_acc: 1.0000 val_acc:0.746432 \n",
            "      time: 0.029\n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "[24] loss: 0.000193\n",
            "      train_acc: 1.0000 val_acc:0.746432 \n",
            "      time: 0.026\n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "[25] loss: 0.000159\n",
            "      train_acc: 1.0000 val_acc:0.744242 \n",
            "      time: 0.035\n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "[26] loss: 0.000133\n",
            "      train_acc: 1.0000 val_acc:0.745146 \n",
            "      time: 0.030\n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "[27] loss: 0.000112\n",
            "      train_acc: 1.0000 val_acc:0.743382 \n",
            "      time: 0.026\n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "[28] loss: 0.000096\n",
            "      train_acc: 1.0000 val_acc:0.747697 \n",
            "      time: 0.026\n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "[29] loss: 0.000083\n",
            "      train_acc: 1.0000 val_acc:0.745933 \n",
            "      time: 0.029\n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "[30] loss: 0.000073\n",
            "      train_acc: 1.0000 val_acc:0.745933 \n",
            "      time: 0.042\n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "[31] loss: 0.000065\n",
            "      train_acc: 1.0000 val_acc:0.748439 \n",
            "      time: 0.030\n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "[32] loss: 0.000059\n",
            "      train_acc: 1.0000 val_acc:0.749344 \n",
            "      time: 0.034\n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "[33] loss: 0.000053\n",
            "      train_acc: 1.0000 val_acc:0.750248 \n",
            "      time: 0.030\n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "[34] loss: 0.000048\n",
            "      train_acc: 1.0000 val_acc:0.750248 \n",
            "      time: 0.028\n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "[35] loss: 0.000044\n",
            "      train_acc: 1.0000 val_acc:0.752754 \n",
            "      time: 0.037\n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "[36] loss: 0.000040\n",
            "      train_acc: 1.0000 val_acc:0.757024 \n",
            "      time: 0.028\n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "[37] loss: 0.000036\n",
            "      train_acc: 1.0000 val_acc:0.759366 \n",
            "      time: 0.029\n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "[38] loss: 0.000033\n",
            "      train_acc: 1.0000 val_acc:0.755956 \n",
            "      time: 0.031\n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "[39] loss: 0.000031\n",
            "      train_acc: 1.0000 val_acc:0.760882 \n",
            "      time: 0.029\n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "[40] loss: 0.000028\n",
            "      train_acc: 1.0000 val_acc:0.756544 \n",
            "      time: 0.030\n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "[41] loss: 0.000026\n",
            "      train_acc: 1.0000 val_acc:0.757448 \n",
            "      time: 0.028\n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "[42] loss: 0.000025\n",
            "      train_acc: 1.0000 val_acc:0.757448 \n",
            "      time: 0.026\n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "[43] loss: 0.000023\n",
            "      train_acc: 1.0000 val_acc:0.755616 \n",
            "      time: 0.020\n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "[44] loss: 0.000022\n",
            "      train_acc: 1.0000 val_acc:0.755616 \n",
            "      time: 0.026\n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "[45] loss: 0.000021\n",
            "      train_acc: 1.0000 val_acc:0.755616 \n",
            "      time: 0.029\n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "[46] loss: 0.000020\n",
            "      train_acc: 1.0000 val_acc:0.755616 \n",
            "      time: 0.020\n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "[47] loss: 0.000019\n",
            "      train_acc: 1.0000 val_acc:0.755616 \n",
            "      time: 0.020\n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "[48] loss: 0.000018\n",
            "      train_acc: 1.0000 val_acc:0.755616 \n",
            "      time: 0.020\n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "[49] loss: 0.000017\n",
            "      train_acc: 1.0000 val_acc:0.755616 \n",
            "      time: 0.020\n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "[50] loss: 0.000016\n",
            "      train_acc: 1.0000 val_acc:0.755616 \n",
            "      time: 0.025\n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "[51] loss: 0.000016\n",
            "      train_acc: 1.0000 val_acc:0.755616 \n",
            "      time: 0.020\n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "[52] loss: 0.000015\n",
            "      train_acc: 1.0000 val_acc:0.755616 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "[53] loss: 0.000014\n",
            "      train_acc: 1.0000 val_acc:0.757380 \n",
            "      time: 0.022\n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "[54] loss: 0.000014\n",
            "      train_acc: 1.0000 val_acc:0.757380 \n",
            "      time: 0.026\n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "[55] loss: 0.000014\n",
            "      train_acc: 1.0000 val_acc:0.757380 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "[56] loss: 0.000013\n",
            "      train_acc: 1.0000 val_acc:0.757380 \n",
            "      time: 0.028\n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "[57] loss: 0.000013\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.022\n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "[58] loss: 0.000012\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "[59] loss: 0.000012\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "[60] loss: 0.000012\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "[61] loss: 0.000012\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.029\n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "[62] loss: 0.000011\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.029\n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "[63] loss: 0.000011\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.031\n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "[64] loss: 0.000011\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.034\n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "[65] loss: 0.000011\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.029\n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "[66] loss: 0.000010\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.030\n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "[67] loss: 0.000010\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.022\n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "[68] loss: 0.000010\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "[69] loss: 0.000010\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.022\n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "[70] loss: 0.000010\n",
            "      train_acc: 1.0000 val_acc:0.755038 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "[71] loss: 0.000010\n",
            "      train_acc: 1.0000 val_acc:0.754134 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "[72] loss: 0.000009\n",
            "      train_acc: 1.0000 val_acc:0.754134 \n",
            "      time: 0.024\n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "[73] loss: 0.000009\n",
            "      train_acc: 1.0000 val_acc:0.754134 \n",
            "      time: 0.022\n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "[74] loss: 0.000009\n",
            "      train_acc: 1.0000 val_acc:0.754134 \n",
            "      time: 0.022\n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "[75] loss: 0.000009\n",
            "      train_acc: 1.0000 val_acc:0.754134 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "[76] loss: 0.000009\n",
            "      train_acc: 1.0000 val_acc:0.754134 \n",
            "      time: 0.022\n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "[77] loss: 0.000009\n",
            "      train_acc: 1.0000 val_acc:0.754134 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "[78] loss: 0.000009\n",
            "      train_acc: 1.0000 val_acc:0.754134 \n",
            "      time: 0.022\n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "[79] loss: 0.000009\n",
            "      train_acc: 1.0000 val_acc:0.754134 \n",
            "      time: 0.020\n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "[80] loss: 0.000008\n",
            "      train_acc: 1.0000 val_acc:0.750166 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "[81] loss: 0.000008\n",
            "      train_acc: 1.0000 val_acc:0.750166 \n",
            "      time: 0.020\n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "[82] loss: 0.000008\n",
            "      train_acc: 1.0000 val_acc:0.750166 \n",
            "      time: 0.020\n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "[83] loss: 0.000008\n",
            "      train_acc: 1.0000 val_acc:0.750166 \n",
            "      time: 0.028\n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "[84] loss: 0.000008\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "[85] loss: 0.000008\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.022\n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "[86] loss: 0.000008\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.024\n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "[87] loss: 0.000008\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.022\n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "[88] loss: 0.000008\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.025\n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "[89] loss: 0.000008\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.024\n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "[90] loss: 0.000007\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.025\n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "[91] loss: 0.000007\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.022\n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "[92] loss: 0.000007\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.023\n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "[93] loss: 0.000007\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "[94] loss: 0.000007\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.023\n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "[95] loss: 0.000007\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "[96] loss: 0.000007\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.023\n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "[97] loss: 0.000007\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.020\n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "[98] loss: 0.000007\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.021\n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "[99] loss: 0.000007\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.019\n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "[100] loss: 0.000007\n",
            "      train_acc: 1.0000 val_acc:0.749262 \n",
            "      time: 0.023\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1, epochs+1), metrics['total_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n",
        "plt.title('Train loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "x9NC64eWXijP",
        "outputId": "c6e2212c-27ec-4331-d53f-3fcb16ce2517"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1, epochs+1), metrics['val_acc'], label='val acc')\n",
        "plt.plot(range(1, epochs+1), metrics['train_acc'], label='train acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "meAJmT2HXj7x",
        "outputId": "c6a20adc-f2d7-4379-e4ba-12e437d3ed82"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = MulticlassAccuracy(num_classes=7)"
      ],
      "metadata": {
        "id": "Tdxp5SjmXm6J"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "outputs = model(data.x, A_norm_tensor)\n",
        "y_pred = torch.argmax(outputs, dim=-1)\n",
        "acc = accuracy(y_pred[data.test_mask], data.y[data.test_mask])\n",
        "\n",
        "print(f'Test accuracy: {acc.cpu():.4f}')\n",
        "print(f'Val accuracy: {metrics[\"val_acc\"][-1]:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUaXLam6Xow4",
        "outputId": "3906c1f1-c9de-46d6-a70d-ce27a4452de6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.7855\n",
            "Val accuracy: 0.7493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb5YjHVClCqo"
      },
      "source": [
        "#### Question 10 (0.5 pt)\n",
        "\n",
        "The paper introduces GCNs as a way to solve a *semi-supervised* classification problem.\n",
        "\n",
        "- What makes this problem semi-supervised?\n",
        "- What is the proportion of labeled data used for training with respect to labeled data in the validation and test sets? What is difference in this context with other benchmark tasks in machine learning, like image classification with MNIST?\n",
        "- Why do you think the GCN performs well in this semi-supervised scenario?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Adjacency matrix can contains some information tha is not present in data. Semi-supervised learning aims to classify unlabeled data by learning the graph structure and labeled data jointly. In our case we were training model on the whole graph, but counting loss only on a subset of known labels.\n",
        "2. In absolute values: train - 20 nodes per class, val - 500 nodes, train - 1000; 5%, 18%, 36% respectively. The proportion of training set is really small in comparison with image classification. Usually we have not less than 75% of data in the training set.\n",
        "3. As we gather the information from the neighbours during learning phase, the unlabeled node also get some info about the labeled neighbour. So the model can understand that if these nodes are connected, they share some information and posibly relate to the same class.\n"
      ],
      "metadata": {
        "id": "7hWyzhXcXyji"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ihrjZddvz5d"
      },
      "source": [
        "### Loading a dataset of proteins\n",
        "\n",
        "In the previous sections you learned how to pass the adjacency matrix of a graph with a couple of thousand of nodes, to classify each node with a particular label. A different and useful application of GCNs is graph classification.\n",
        "\n",
        "In contrast with the previous part, where there was a single, big graph, in graph classification we have multiple graphs, and each graph can be assigned a label. In this part of the assignment you will implement a classifier for proteins.\n",
        "\n",
        "[Proteins](https://en.wikipedia.org/wiki/Protein_(nutrient)) are parts of the buildings block of life. They consist of chains of amino acids, and can take many shapes. In the PROTEINS dataset, proteins are represented as graphs, where the nodes are amino acids, and an edge between them indicates that they are 6 [Angstroms](https://en.wikipedia.org/wiki/Angstrom) apart. All graphs have a binary label, where 1 means that the protein is not an enzyme.\n",
        "\n",
        "We will start by loading and examining this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmqweMcvnUH6"
      },
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "dataset = TUDataset(root='data/TU', name='PROTEINS', use_node_attr=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oF1gyKPXiz-"
      },
      "source": [
        "#### Question 11 (0.25 pt)\n",
        "\n",
        "Unlike in the previous part, where we selected the first element returned by the loading function, note that here we get all the elements returned by `TUDataset()`. `dataset` is an interable object, that has some similar behaviors as a Python list: you can call `len()` on it, and you can takes slices from it.\n",
        "\n",
        "Each element in `dataset` is a `Data` object containing a graph that represents a protein. This is the same type of object that we used in the previous part to store the Cora citation network.\n",
        "\n",
        "Knowing this, answer the following:\n",
        "\n",
        "- How many proteins (graphs) are there in `dataset`?\n",
        "- Take any protein from `dataset`. How many nodes and edges does it contain? What is its label? How many features does each node have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNPsnXXbbHHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba599ec8-383f-4569-bf92-febe81b21fcb"
      },
      "source": [
        "print(f'Amount of proteins in dataset: {len(dataset)}')\n",
        "print(f'Amount of nodes in protein: {dataset[0].num_nodes}')\n",
        "print(f'Amount of edges in protein: {dataset[0].num_edges}')\n",
        "print(f'Amount of features per node in protein: {dataset[0].num_node_features}')\n",
        "print(f'Protein\\'s label: {dataset[0].y}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of proteins in dataset: 1113\n",
            "Amount of nodes in protein: 42\n",
            "Amount of edges in protein: 162\n",
            "Amount of features per node in protein: 4\n",
            "Protein's label: tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHSklBZXpKpR"
      },
      "source": [
        "#### Question 12 (0.5 pt)\n",
        "\n",
        "To properly train and evaluate our model, we need training, validation, and test splits.\n",
        "\n",
        "For reproducibility purposes, we generate a random tensor of indices for you. Use it to extract the three splits from `dataset`.\n",
        "\n",
        "For training, take 80% of the indices (starting from the first element in `indices`), then the following 10% for validation, and the remaining 10% for testing. You can use the indices to index `dataset`.\n",
        "\n",
        "Call the resulting splits `train_dataset`, `valid_dataset`, and `test_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttY4d1GInn08"
      },
      "source": [
        "# Don't erase the following three lines\n",
        "import torch\n",
        "torch.random.manual_seed(0)\n",
        "indices = torch.randperm(len(dataset))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFSeWpuCYxzW",
        "outputId": "bb7a34b3-a3ac-46b2-d6f4-451860efc214"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 158,  648, 1026,  ...,  513,  295,   29])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = [dataset[index] for index in indices[:round(0.8 * len(indices))]]\n",
        "valid_dataset = [dataset[index] for index in indices[round(0.8 * len(indices)) : round(0.8 * len(indices)) + round(0.1 * len(indices))]]\n",
        "test_dataset  = [dataset[index] for index in indices[round(0.8 * len(indices)) + round(0.1 * len(indices)):]]"
      ],
      "metadata": {
        "id": "6JrkPtgxYA7F"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(train_dataset) + len(valid_dataset) + len(test_dataset) == len(dataset)"
      ],
      "metadata": {
        "id": "JxqY3sYpYBpO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDJbB4CQqsfp"
      },
      "source": [
        "### Working with a batch of graphs\n",
        "\n",
        "When working with the Cora dataset, you used the information in `data.edge_index` to build the sparse normalized adjacency matrix $\\hat{A}$ that is required by the GCN. We could do something similar here: for each graph, we build $\\hat{A}$, and pass it to the GCN. However, if the number of graphs is big, this can really slow down training.\n",
        "\n",
        "To avoid this, we will resort to a very useful trick that also allows us to reuse the same GCN you implemented previously. The trick makes it possible to do a forward pass through the GCN for multiple, disconnected graphs at the same time (instead of only one), much like when you train with mini-batches for other kinds of data.\n",
        "\n",
        "Let's first revisit the propagation rule of the GCN, $Z = \\hat{A}XW$, with an illustration (we have omitted the cells of $X$ and $W$ for clarity):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/02-gcn-forward.png\">\n",
        "\n",
        "If we have multiple graphs, we can still use the same propagation rule, if we\n",
        "\n",
        "- Set $\\hat{A}$ to be a block diagonal matrix, where the blocks are the different adjacency matrices of the graphs\n",
        "- Concatenate the feature matrices along the first dimension\n",
        "\n",
        "This is illustrated in the following figure, for a batch of 3 graphs. Note that the elements outside of the blocks are zero.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/02-gcn-batch-forward.png\">\n",
        "\n",
        "The resulting adjacency matrix $\\hat{A}_B$ can also be built as a sparse matrix, and once we have it together with the concatenated matrix of features, the computation of the graph convolution is exactly the same as before. Note how this trick also allows us to process graphs with different sizes and structures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DLPJ62b2mQ6"
      },
      "source": [
        "#### Question 13 (0.5 pt)\n",
        "\n",
        "\n",
        "Just as the citation network, the graphs in each of the datasets you created in Question 12 also have an `edge_index` attribute, which can be used to compute the normalized adjacency matrix $\\hat{A}$, for each graph.\n",
        "\n",
        "Reusing your code for Questions 3 and 5, define a function `get_a_norm()` that takes as input an element of a dataset (e.g. `train_dataset[0]`), and returns a `scipy.sparse` matrix containing $\\hat{A}$.\n",
        "\n",
        "Note that an element of a dataset has properties like `num_edges`, `num_nodes`, etc. which you can use here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nvPX2GB8oXp"
      },
      "source": [
        "def get_a_norm(data):\n",
        "  # adjacency matrix\n",
        "  rows = np.array(data.edge_index[0])\n",
        "  cols = np.array(data.edge_index[1])\n",
        "  d = np.ones(cols.shape)\n",
        "  adj_matrix = coo_matrix((d, (rows, cols)), shape=data.size())\n",
        "\n",
        "  I = np.eye(adj_matrix.shape[0])\n",
        "  A_ = adj_matrix.toarray() + I\n",
        "  D_ = diags(sum(A_), dtype=np.float32)\n",
        "  D_inv = fractional_matrix_power(D_.toarray(), -1/2)\n",
        "\n",
        "  A_norm = np.matmul(np.matmul(D_inv, A_), D_inv)\n",
        "  return coo_matrix(A_norm)\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_a_norm(train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA1N-s_JZd1I",
        "outputId": "0dfab647-cec2-4b0b-a75e-5df9cdf03602"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<31x31 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 153 stored elements in COOrdinate format>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBrmYBY3AfhW"
      },
      "source": [
        "#### Question 14 (1 pt)\n",
        "\n",
        "To prepare the batch of graphs, we need to collect multiple adjacency matrices, feature matrices, and labels.\n",
        "\n",
        "When using the trick described in the last figure, we see that we have to keep track of when a graph starts and when it ends, so that we can later differentiate the outputs due to $X^{(0)}$, $X^{(1)}$, etc. To achieve this, we will additionally collect a 1D array of batch indices, one for each $X^{(i)}$.\n",
        "\n",
        "The 1D array has as many elements as rows in $X^{(i)}$, and it is filled with the value $i$ (the position of $X^{(i)}$ in the batch):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/03-batch-indices.png\">\n",
        "\n",
        "We will later concatenate all the 1D arrays along the first dimension, just as we will do with all the $X^{(i)}$.\n",
        "\n",
        "Define a function `prepare_graphs_batch()` that takes as input a dataset (e.g. `train_dataset`), and does the following\n",
        "\n",
        "- Create four empty lists:\n",
        "  - `adj_matrices`\n",
        "  - `feature_matrices`\n",
        "  - `batch_indices`\n",
        "  - `labels`\n",
        "- Iterate over the input dataset, getting one graph at a time. At each step, use your function from Question 13 to append the adjacency matrix to `adj_matrices`, append the matrix of input features to `feature_matrices`, create the array of batch indices (as explained above) and append it to `batch_indices`, and append the label of the graph to `labels`. **Make sure to convert the label to float**.\n",
        "- Once the loop is over, use `scipy.sparse.block_diag()` to build the block diagonal matrix $\\hat{A}_B$. Convert it to the COO format, and then use your answer to Question 6 to turn it into a sparse PyTorch tensor.\n",
        "- Use `torch.cat()` to concatenate the tensors in `feature_matrices` along the first dimension. Do this also for `batch_indices` and `labels`.\n",
        "- Return the 4 tensors computed in the previous two items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsQ0-JjSqFgD"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from scipy.sparse import block_diag"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_graphs_batch(dataset):\n",
        "  adj_matrices = []\n",
        "  feature_matrices = []\n",
        "  batch_indices = []\n",
        "  labels = []\n",
        "\n",
        "  for idx, item in tqdm(enumerate(dataset)):\n",
        "    adj_matrices.append(get_a_norm(item))\n",
        "    feature_matrices.append(item.x)\n",
        "    batch_indices.append(torch.tensor(idx).repeat(item.num_nodes))\n",
        "    labels.append(item.y.float())\n",
        "\n",
        "  # create block adj norm matrix\n",
        "  adj_matrices = block_diag(adj_matrices)\n",
        "  indices = np.array((adj_matrices.row, adj_matrices.col))\n",
        "  adj_matrices = torch.sparse_coo_tensor(indices=indices, values=adj_matrices.data, \n",
        "                                         size=adj_matrices.shape, dtype=torch.float32)\n",
        "\n",
        "  feature_matrices = torch.cat(feature_matrices)\n",
        "  batch_indices = torch.cat(batch_indices)\n",
        "  labels = torch.cat(labels)\n",
        "\n",
        "  return adj_matrices, feature_matrices, batch_indices, labels\n"
      ],
      "metadata": {
        "id": "y6T4VIiPac1r"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i73P_EU0MSPX"
      },
      "source": [
        "Once your answer for the previous question is ready, you can run the next cell to prepare all the required information, for the train, validation, and test splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iol5FxJGMmAU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "6e38d3667ff54170a10908e74cf22e32",
            "5dca77e5bd284b509cbc4fd1f7c944e1",
            "7958830441544c4aab5236996ce3f52a",
            "dfecafec186e40bab904eb619ea107ca",
            "67f945ec7580469b95ef6463e5563b11",
            "da19d8d9336a49e6a1720f04ddaa4aee",
            "3ebfb5764c524bbeba9394088f06bc7b",
            "b951b7319e3543f8bba6bc26b3b19de8",
            "1e761c7b631640a89a26e7155aeac3c6",
            "7df5e4137419407fa4c80804670a4211",
            "3a3284d7f5fb4566bc6920e56f16d59d",
            "2f9e9ae48abc41639ccc31eb7d6ddda8",
            "af75d1a4b05d43aa827e3590e1e75339",
            "2a3aa766da55436fa140963e95ec956c",
            "a390c2dba18a4ad2bca2548bd07fc5dc",
            "a7bc795862324942bf869290ce138852",
            "ac4be491f8fe41c28aa2fd17d58f7440",
            "50fc3dd7ac5d40a0896d5df51ec853c5",
            "914f3f011aa94a339d84c593ebd6cff3",
            "9f8cb4ba720d4d99ab0522e213c2d3cb",
            "aaca3fd654d84a3aa8eaf0f51d34dda8",
            "607e0a818b8d4739a2835f6cef6c8360",
            "dd6ffc9b8aff4df0b3ef4ccb68ef5119",
            "972073bb67234d9c9416874984241cd4",
            "1193e220c5084ecaad7070a60f64791e",
            "393506cb51484cdebe4672c6616ffe36",
            "b2b144d6744f4e4c800bb9392fde4f35",
            "c4615605b09f4218ae4eabce46cd0f84",
            "a77a8b0b79014ebfbda20179f7bf08bf",
            "970470e333324fb486c8d6443f615416",
            "20e1d12eda9b4ee3ad4ebb2f99a29d80",
            "1093d191dae54bd5b27521c71eb67a42",
            "a5462f3e4fc34d6daef49901012dd41c"
          ]
        },
        "outputId": "f361db10-1a7f-4b34-f9c4-d48ab344e6a6"
      },
      "source": [
        "train_a_norm, train_features, train_batch_idx, train_labels = prepare_graphs_batch(train_dataset)\n",
        "valid_a_norm, valid_features, valid_batch_idx, valid_labels = prepare_graphs_batch(valid_dataset)\n",
        "test_a_norm, test_features, test_batch_idx, test_labels = prepare_graphs_batch(test_dataset)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e38d3667ff54170a10908e74cf22e32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f9e9ae48abc41639ccc31eb7d6ddda8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd6ffc9b8aff4df0b3ef4ccb68ef5119"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features.size(), test_batch_idx.size(), test_labels.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehLk7Kzwaqyh",
        "outputId": "b0422197-551e-4ce0-8a03-954e7741cc2e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4628, 4]), torch.Size([4628]), torch.Size([112]))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6q-JU87NClh"
      },
      "source": [
        "### GCNs for graph classification\n",
        "\n",
        "We now have all the ingredients to pass a batch of graphs to a GCN. However, for each graph in the batch, the output $Z^{(i)}$ contains one row for each node in the graph. If the goal is to do classification at the graph level, we have to *pool* these vectors to then compute the required logits for classification.\n",
        "\n",
        "This operation is similar as how pooling works in a CNN. We could consider taking the mean of the vectors, the sum, or use max-pooling. The difference with respect to CNNs is that in our case, we have a batch of graphs, each potentially with a different number of nodes.\n",
        "\n",
        "To implement this specific pooling, we can use the scatter operation in the `torch_scatter` library, which comes when installing PyG. We will use it, together with the tensor of batch indices from the previous two questions, to pool the outputs of the GCN for each graph, into a single vector:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/04-scatter.png\">\n",
        "\n",
        "You can check more details in the [documentation](https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY87DX1uRhnY"
      },
      "source": [
        "#### Question 15 (1.0 pt)\n",
        "\n",
        "Implement a `GraphClassifier` module using PyTorch.\n",
        "\n",
        "- The constructor should take as arguments the number of input features, the hidden dimension, and the number of classes.\n",
        "- The model should contain a instance of the `GCN` module (as you implemented it in Question 8). Use the same value for the hidden dimension and the number of output features (recall that your `GCN` module from Question 8 has two GCN layers).\n",
        "- The model should also contain a `torch.nn.Linear` layer, with the hidden dimension as the input features, and the number of classes as the output.\n",
        "- The forward method receives the concatenated matrix of features, the sparse block diagonal adjacency matrix, and the batch indices (the latter is used when calling `scatter`).\n",
        "- Use the following architecture in the forward pass:\n",
        "  - GCN $\\to$ ReLU $\\to$ scatter (max) $\\to$ Linear.\n",
        "\n",
        "The output of the forward should be a 1D tensor (you might need to call `squeeze` to get rid of extra dimensions) containing the logits for all graphs in the batch, for the binary classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "750WraywwYDH"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch_scatter"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphClassifier(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size)-> None:\n",
        "    super().__init__()\n",
        "    \n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.gcn = GCN(self.input_size, self.hidden_size, self.hidden_size)\n",
        "    self.relu   = nn.ReLU()\n",
        "    self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
        "  \n",
        "  def forward(self, x, a_norm, batch_idx):\n",
        "    x = self.gcn(x, a_norm)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = x.transpose(0, 1)\n",
        "    x = torch_scatter.scatter(src=x, index=batch_idx, reduce='max')\n",
        "    x = x.transpose(0, 1)\n",
        "\n",
        "    x = self.linear(x)\n",
        "    x = x.squeeze(1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "9It4ODs2axc3"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1PHy-_vTjgh"
      },
      "source": [
        "#### Question 16 (1.5 pt)\n",
        "\n",
        "Implement a training loop for the graph classifier. Use the data from Question 14 to train and evaluate the model.\n",
        "\n",
        "We encourage you to use a GPU in this section for faster training. Note that if you change the runtime at this point, you must re-execute several of the cells above, including the ones that install PyG.\n",
        "\n",
        "- Instantiate a classifier with 32 as the hidden dimension\n",
        "- Use Adam with a learning rate of 1e-3.\n",
        "- Use `torch.nn.BCEWithLogitsLoss` as the loss function.\n",
        "- Train for 5,000 epochs. Once training is done, plot the loss curve and the accuracy in the validation set. Then report the accuracy in the test set.\n",
        "\n",
        "**Note:** the logits from the output of the classifier come from a linear layer. To compute actual predictions for the calculation of the accuracy, pass the logits through `torch.sigmoid()`, and set the predicted values to 1 whenever they are greater than 0.5, and to 0 otherwise.\n",
        "\n",
        "You should get an accuracy equal to or higher than 70% in the validation and test sets. Can you beat the [state-of-the-art](https://paperswithcode.com/sota/graph-classification-on-proteins)? Feel free to modify your architecture and experiment with it.\n",
        "\n",
        "Discuss what you observe during training and your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DbGAs8W2Xja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88936a56-f9dc-4609-c14b-681b93e72e92"
      },
      "source": [
        "# If your runtime is GPU-enabled, use .to(device) to move the model\n",
        "# and all the relevant tensors to the GPU. You have to move tensors back to CPU\n",
        "# when computing metrics like accuracy, using .cpu().\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.classification import BinaryAccuracy"
      ],
      "metadata": {
        "id": "SdC1Qm1Aa5WE"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gc_train(model, X_train, y_train, train_batch_idx, train_a_norm,\n",
        "             X_val, y_val, val_batch_idx, valid_a_norm,\n",
        "             device, learning_rate=0.001, epochs=5000):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  accuracy = BinaryAccuracy(threshold=0.5).to(device) #  If preds is a floating point tensor with values outside [0,1] range we consider the input to be logits and will auto apply sigmoid per element.\n",
        "\n",
        "  X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "  X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "  train_a_norm, valid_a_norm = train_a_norm.to(device), valid_a_norm.to(device)\n",
        "  train_batch_idx, val_batch_idx = train_batch_idx.to(device), val_batch_idx.to(device)\n",
        "\n",
        "  model.to(device)\n",
        "  \n",
        "  metrics = {\n",
        "    'total_loss': [],\n",
        "    'val_acc': [],\n",
        "    'train_acc': []\n",
        "  }\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
        "\n",
        "    # Train part\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    start_time = time()\n",
        "    outputs = model(X_train, train_a_norm, train_batch_idx)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    finish_time = time()\n",
        "\n",
        "    # Evaluation part\n",
        "    model.eval()\n",
        "\n",
        "    val_outputs = model(X_val, valid_a_norm, val_batch_idx)\n",
        "\n",
        "    metrics['total_loss'].append(loss.item())\n",
        "    metrics['train_acc'].append(accuracy(outputs, y_train).item())\n",
        "    metrics['val_acc'].append(accuracy(val_outputs, y_val).item())\n",
        "\n",
        "    print(f'''[{epoch + 1}] loss: {loss.item() / X_train.size()[0]:.6f}\n",
        "      train_acc: {metrics['train_acc'][-1]:.4f} val_acc:{metrics['val_acc'][-1]:4f} \n",
        "      time: {finish_time - start_time:.3f}\\n''')\n",
        "\n",
        "  return model, metrics"
      ],
      "metadata": {
        "id": "Nd_YX7wta2-y"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = train_features.size()[1]\n",
        "hidden_size = 32\n",
        "output_size = 1 # binary classification\n",
        "\n",
        "learning_rate = 1e-3\n",
        "epochs = 5000"
      ],
      "metadata": {
        "id": "fDOzfAiHa_5q"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GraphClassifier(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "nuPlFR7qbBGe"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, metrics = gc_train(\n",
        "    model, train_features, train_labels, train_batch_idx, train_a_norm,\n",
        "    valid_features, valid_labels, valid_batch_idx, valid_a_norm,\n",
        "    device, learning_rate, epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzHq84LAbC8a",
        "outputId": "06d0a673-c1a0-49b0-afa7-0b519c3772e4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4168\n",
            "-------------------------------\n",
            "[4168] loss: 0.000012\n",
            "      train_acc: 0.8348 val_acc:0.855856 \n",
            "      time: 0.008\n",
            "\n",
            "Epoch 4169\n",
            "-------------------------------\n",
            "[4169] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4170\n",
            "-------------------------------\n",
            "[4170] loss: 0.000012\n",
            "      train_acc: 0.8348 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4171\n",
            "-------------------------------\n",
            "[4171] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4172\n",
            "-------------------------------\n",
            "[4172] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4173\n",
            "-------------------------------\n",
            "[4173] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4174\n",
            "-------------------------------\n",
            "[4174] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4175\n",
            "-------------------------------\n",
            "[4175] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4176\n",
            "-------------------------------\n",
            "[4176] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4177\n",
            "-------------------------------\n",
            "[4177] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4178\n",
            "-------------------------------\n",
            "[4178] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4179\n",
            "-------------------------------\n",
            "[4179] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4180\n",
            "-------------------------------\n",
            "[4180] loss: 0.000012\n",
            "      train_acc: 0.8348 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4181\n",
            "-------------------------------\n",
            "[4181] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4182\n",
            "-------------------------------\n",
            "[4182] loss: 0.000012\n",
            "      train_acc: 0.8348 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4183\n",
            "-------------------------------\n",
            "[4183] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.873874 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4184\n",
            "-------------------------------\n",
            "[4184] loss: 0.000012\n",
            "      train_acc: 0.8326 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4185\n",
            "-------------------------------\n",
            "[4185] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.011\n",
            "\n",
            "Epoch 4186\n",
            "-------------------------------\n",
            "[4186] loss: 0.000012\n",
            "      train_acc: 0.8348 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4187\n",
            "-------------------------------\n",
            "[4187] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4188\n",
            "-------------------------------\n",
            "[4188] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4189\n",
            "-------------------------------\n",
            "[4189] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4190\n",
            "-------------------------------\n",
            "[4190] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4191\n",
            "-------------------------------\n",
            "[4191] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4192\n",
            "-------------------------------\n",
            "[4192] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.873874 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4193\n",
            "-------------------------------\n",
            "[4193] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4194\n",
            "-------------------------------\n",
            "[4194] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4195\n",
            "-------------------------------\n",
            "[4195] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4196\n",
            "-------------------------------\n",
            "[4196] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.873874 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4197\n",
            "-------------------------------\n",
            "[4197] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4198\n",
            "-------------------------------\n",
            "[4198] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4199\n",
            "-------------------------------\n",
            "[4199] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4200\n",
            "-------------------------------\n",
            "[4200] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4201\n",
            "-------------------------------\n",
            "[4201] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4202\n",
            "-------------------------------\n",
            "[4202] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4203\n",
            "-------------------------------\n",
            "[4203] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4204\n",
            "-------------------------------\n",
            "[4204] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4205\n",
            "-------------------------------\n",
            "[4205] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4206\n",
            "-------------------------------\n",
            "[4206] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4207\n",
            "-------------------------------\n",
            "[4207] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.008\n",
            "\n",
            "Epoch 4208\n",
            "-------------------------------\n",
            "[4208] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4209\n",
            "-------------------------------\n",
            "[4209] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4210\n",
            "-------------------------------\n",
            "[4210] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4211\n",
            "-------------------------------\n",
            "[4211] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4212\n",
            "-------------------------------\n",
            "[4212] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4213\n",
            "-------------------------------\n",
            "[4213] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4214\n",
            "-------------------------------\n",
            "[4214] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4215\n",
            "-------------------------------\n",
            "[4215] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4216\n",
            "-------------------------------\n",
            "[4216] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4217\n",
            "-------------------------------\n",
            "[4217] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4218\n",
            "-------------------------------\n",
            "[4218] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4219\n",
            "-------------------------------\n",
            "[4219] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4220\n",
            "-------------------------------\n",
            "[4220] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4221\n",
            "-------------------------------\n",
            "[4221] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4222\n",
            "-------------------------------\n",
            "[4222] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4223\n",
            "-------------------------------\n",
            "[4223] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4224\n",
            "-------------------------------\n",
            "[4224] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4225\n",
            "-------------------------------\n",
            "[4225] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4226\n",
            "-------------------------------\n",
            "[4226] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4227\n",
            "-------------------------------\n",
            "[4227] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4228\n",
            "-------------------------------\n",
            "[4228] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4229\n",
            "-------------------------------\n",
            "[4229] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4230\n",
            "-------------------------------\n",
            "[4230] loss: 0.000012\n",
            "      train_acc: 0.8348 val_acc:0.873874 \n",
            "      time: 0.007\n",
            "\n",
            "Epoch 4231\n",
            "-------------------------------\n",
            "[4231] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.873874 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4232\n",
            "-------------------------------\n",
            "[4232] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4233\n",
            "-------------------------------\n",
            "[4233] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.873874 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4234\n",
            "-------------------------------\n",
            "[4234] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4235\n",
            "-------------------------------\n",
            "[4235] loss: 0.000012\n",
            "      train_acc: 0.8348 val_acc:0.873874 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4236\n",
            "-------------------------------\n",
            "[4236] loss: 0.000012\n",
            "      train_acc: 0.8337 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4237\n",
            "-------------------------------\n",
            "[4237] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.873874 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4238\n",
            "-------------------------------\n",
            "[4238] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4239\n",
            "-------------------------------\n",
            "[4239] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4240\n",
            "-------------------------------\n",
            "[4240] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4241\n",
            "-------------------------------\n",
            "[4241] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4242\n",
            "-------------------------------\n",
            "[4242] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4243\n",
            "-------------------------------\n",
            "[4243] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4244\n",
            "-------------------------------\n",
            "[4244] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4245\n",
            "-------------------------------\n",
            "[4245] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4246\n",
            "-------------------------------\n",
            "[4246] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4247\n",
            "-------------------------------\n",
            "[4247] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4248\n",
            "-------------------------------\n",
            "[4248] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4249\n",
            "-------------------------------\n",
            "[4249] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4250\n",
            "-------------------------------\n",
            "[4250] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4251\n",
            "-------------------------------\n",
            "[4251] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4252\n",
            "-------------------------------\n",
            "[4252] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4253\n",
            "-------------------------------\n",
            "[4253] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4254\n",
            "-------------------------------\n",
            "[4254] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4255\n",
            "-------------------------------\n",
            "[4255] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4256\n",
            "-------------------------------\n",
            "[4256] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.009\n",
            "\n",
            "Epoch 4257\n",
            "-------------------------------\n",
            "[4257] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4258\n",
            "-------------------------------\n",
            "[4258] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4259\n",
            "-------------------------------\n",
            "[4259] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4260\n",
            "-------------------------------\n",
            "[4260] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4261\n",
            "-------------------------------\n",
            "[4261] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4262\n",
            "-------------------------------\n",
            "[4262] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4263\n",
            "-------------------------------\n",
            "[4263] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4264\n",
            "-------------------------------\n",
            "[4264] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4265\n",
            "-------------------------------\n",
            "[4265] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4266\n",
            "-------------------------------\n",
            "[4266] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4267\n",
            "-------------------------------\n",
            "[4267] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4268\n",
            "-------------------------------\n",
            "[4268] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.873874 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4269\n",
            "-------------------------------\n",
            "[4269] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4270\n",
            "-------------------------------\n",
            "[4270] loss: 0.000012\n",
            "      train_acc: 0.8348 val_acc:0.873874 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4271\n",
            "-------------------------------\n",
            "[4271] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4272\n",
            "-------------------------------\n",
            "[4272] loss: 0.000012\n",
            "      train_acc: 0.8348 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4273\n",
            "-------------------------------\n",
            "[4273] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4274\n",
            "-------------------------------\n",
            "[4274] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4275\n",
            "-------------------------------\n",
            "[4275] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4276\n",
            "-------------------------------\n",
            "[4276] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.008\n",
            "\n",
            "Epoch 4277\n",
            "-------------------------------\n",
            "[4277] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.007\n",
            "\n",
            "Epoch 4278\n",
            "-------------------------------\n",
            "[4278] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4279\n",
            "-------------------------------\n",
            "[4279] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4280\n",
            "-------------------------------\n",
            "[4280] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4281\n",
            "-------------------------------\n",
            "[4281] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4282\n",
            "-------------------------------\n",
            "[4282] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4283\n",
            "-------------------------------\n",
            "[4283] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4284\n",
            "-------------------------------\n",
            "[4284] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4285\n",
            "-------------------------------\n",
            "[4285] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.007\n",
            "\n",
            "Epoch 4286\n",
            "-------------------------------\n",
            "[4286] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4287\n",
            "-------------------------------\n",
            "[4287] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4288\n",
            "-------------------------------\n",
            "[4288] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4289\n",
            "-------------------------------\n",
            "[4289] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4290\n",
            "-------------------------------\n",
            "[4290] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4291\n",
            "-------------------------------\n",
            "[4291] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4292\n",
            "-------------------------------\n",
            "[4292] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4293\n",
            "-------------------------------\n",
            "[4293] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4294\n",
            "-------------------------------\n",
            "[4294] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4295\n",
            "-------------------------------\n",
            "[4295] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4296\n",
            "-------------------------------\n",
            "[4296] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4297\n",
            "-------------------------------\n",
            "[4297] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4298\n",
            "-------------------------------\n",
            "[4298] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4299\n",
            "-------------------------------\n",
            "[4299] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4300\n",
            "-------------------------------\n",
            "[4300] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4301\n",
            "-------------------------------\n",
            "[4301] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4302\n",
            "-------------------------------\n",
            "[4302] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4303\n",
            "-------------------------------\n",
            "[4303] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4304\n",
            "-------------------------------\n",
            "[4304] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4305\n",
            "-------------------------------\n",
            "[4305] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4306\n",
            "-------------------------------\n",
            "[4306] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4307\n",
            "-------------------------------\n",
            "[4307] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4308\n",
            "-------------------------------\n",
            "[4308] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4309\n",
            "-------------------------------\n",
            "[4309] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4310\n",
            "-------------------------------\n",
            "[4310] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4311\n",
            "-------------------------------\n",
            "[4311] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4312\n",
            "-------------------------------\n",
            "[4312] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4313\n",
            "-------------------------------\n",
            "[4313] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4314\n",
            "-------------------------------\n",
            "[4314] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4315\n",
            "-------------------------------\n",
            "[4315] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4316\n",
            "-------------------------------\n",
            "[4316] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4317\n",
            "-------------------------------\n",
            "[4317] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4318\n",
            "-------------------------------\n",
            "[4318] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4319\n",
            "-------------------------------\n",
            "[4319] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4320\n",
            "-------------------------------\n",
            "[4320] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.008\n",
            "\n",
            "Epoch 4321\n",
            "-------------------------------\n",
            "[4321] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4322\n",
            "-------------------------------\n",
            "[4322] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4323\n",
            "-------------------------------\n",
            "[4323] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4324\n",
            "-------------------------------\n",
            "[4324] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4325\n",
            "-------------------------------\n",
            "[4325] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4326\n",
            "-------------------------------\n",
            "[4326] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4327\n",
            "-------------------------------\n",
            "[4327] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4328\n",
            "-------------------------------\n",
            "[4328] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4329\n",
            "-------------------------------\n",
            "[4329] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4330\n",
            "-------------------------------\n",
            "[4330] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4331\n",
            "-------------------------------\n",
            "[4331] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4332\n",
            "-------------------------------\n",
            "[4332] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4333\n",
            "-------------------------------\n",
            "[4333] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4334\n",
            "-------------------------------\n",
            "[4334] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4335\n",
            "-------------------------------\n",
            "[4335] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4336\n",
            "-------------------------------\n",
            "[4336] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4337\n",
            "-------------------------------\n",
            "[4337] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4338\n",
            "-------------------------------\n",
            "[4338] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4339\n",
            "-------------------------------\n",
            "[4339] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4340\n",
            "-------------------------------\n",
            "[4340] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4341\n",
            "-------------------------------\n",
            "[4341] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4342\n",
            "-------------------------------\n",
            "[4342] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4343\n",
            "-------------------------------\n",
            "[4343] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4344\n",
            "-------------------------------\n",
            "[4344] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4345\n",
            "-------------------------------\n",
            "[4345] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4346\n",
            "-------------------------------\n",
            "[4346] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4347\n",
            "-------------------------------\n",
            "[4347] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4348\n",
            "-------------------------------\n",
            "[4348] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4349\n",
            "-------------------------------\n",
            "[4349] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4350\n",
            "-------------------------------\n",
            "[4350] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4351\n",
            "-------------------------------\n",
            "[4351] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4352\n",
            "-------------------------------\n",
            "[4352] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4353\n",
            "-------------------------------\n",
            "[4353] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4354\n",
            "-------------------------------\n",
            "[4354] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4355\n",
            "-------------------------------\n",
            "[4355] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4356\n",
            "-------------------------------\n",
            "[4356] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4357\n",
            "-------------------------------\n",
            "[4357] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4358\n",
            "-------------------------------\n",
            "[4358] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4359\n",
            "-------------------------------\n",
            "[4359] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4360\n",
            "-------------------------------\n",
            "[4360] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.007\n",
            "\n",
            "Epoch 4361\n",
            "-------------------------------\n",
            "[4361] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4362\n",
            "-------------------------------\n",
            "[4362] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4363\n",
            "-------------------------------\n",
            "[4363] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4364\n",
            "-------------------------------\n",
            "[4364] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4365\n",
            "-------------------------------\n",
            "[4365] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4366\n",
            "-------------------------------\n",
            "[4366] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4367\n",
            "-------------------------------\n",
            "[4367] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4368\n",
            "-------------------------------\n",
            "[4368] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4369\n",
            "-------------------------------\n",
            "[4369] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4370\n",
            "-------------------------------\n",
            "[4370] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4371\n",
            "-------------------------------\n",
            "[4371] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4372\n",
            "-------------------------------\n",
            "[4372] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4373\n",
            "-------------------------------\n",
            "[4373] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4374\n",
            "-------------------------------\n",
            "[4374] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4375\n",
            "-------------------------------\n",
            "[4375] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4376\n",
            "-------------------------------\n",
            "[4376] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4377\n",
            "-------------------------------\n",
            "[4377] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4378\n",
            "-------------------------------\n",
            "[4378] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4379\n",
            "-------------------------------\n",
            "[4379] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4380\n",
            "-------------------------------\n",
            "[4380] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4381\n",
            "-------------------------------\n",
            "[4381] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4382\n",
            "-------------------------------\n",
            "[4382] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4383\n",
            "-------------------------------\n",
            "[4383] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4384\n",
            "-------------------------------\n",
            "[4384] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4385\n",
            "-------------------------------\n",
            "[4385] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4386\n",
            "-------------------------------\n",
            "[4386] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4387\n",
            "-------------------------------\n",
            "[4387] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4388\n",
            "-------------------------------\n",
            "[4388] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4389\n",
            "-------------------------------\n",
            "[4389] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4390\n",
            "-------------------------------\n",
            "[4390] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4391\n",
            "-------------------------------\n",
            "[4391] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4392\n",
            "-------------------------------\n",
            "[4392] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4393\n",
            "-------------------------------\n",
            "[4393] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4394\n",
            "-------------------------------\n",
            "[4394] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4395\n",
            "-------------------------------\n",
            "[4395] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4396\n",
            "-------------------------------\n",
            "[4396] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4397\n",
            "-------------------------------\n",
            "[4397] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4398\n",
            "-------------------------------\n",
            "[4398] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4399\n",
            "-------------------------------\n",
            "[4399] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4400\n",
            "-------------------------------\n",
            "[4400] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4401\n",
            "-------------------------------\n",
            "[4401] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4402\n",
            "-------------------------------\n",
            "[4402] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4403\n",
            "-------------------------------\n",
            "[4403] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4404\n",
            "-------------------------------\n",
            "[4404] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4405\n",
            "-------------------------------\n",
            "[4405] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4406\n",
            "-------------------------------\n",
            "[4406] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4407\n",
            "-------------------------------\n",
            "[4407] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4408\n",
            "-------------------------------\n",
            "[4408] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4409\n",
            "-------------------------------\n",
            "[4409] loss: 0.000012\n",
            "      train_acc: 0.8326 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4410\n",
            "-------------------------------\n",
            "[4410] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4411\n",
            "-------------------------------\n",
            "[4411] loss: 0.000012\n",
            "      train_acc: 0.8326 val_acc:0.864865 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4412\n",
            "-------------------------------\n",
            "[4412] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4413\n",
            "-------------------------------\n",
            "[4413] loss: 0.000012\n",
            "      train_acc: 0.8315 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4414\n",
            "-------------------------------\n",
            "[4414] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4415\n",
            "-------------------------------\n",
            "[4415] loss: 0.000012\n",
            "      train_acc: 0.8326 val_acc:0.864865 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4416\n",
            "-------------------------------\n",
            "[4416] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4417\n",
            "-------------------------------\n",
            "[4417] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4418\n",
            "-------------------------------\n",
            "[4418] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4419\n",
            "-------------------------------\n",
            "[4419] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.011\n",
            "\n",
            "Epoch 4420\n",
            "-------------------------------\n",
            "[4420] loss: 0.000012\n",
            "      train_acc: 0.8360 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4421\n",
            "-------------------------------\n",
            "[4421] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4422\n",
            "-------------------------------\n",
            "[4422] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4423\n",
            "-------------------------------\n",
            "[4423] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4424\n",
            "-------------------------------\n",
            "[4424] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4425\n",
            "-------------------------------\n",
            "[4425] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4426\n",
            "-------------------------------\n",
            "[4426] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4427\n",
            "-------------------------------\n",
            "[4427] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4428\n",
            "-------------------------------\n",
            "[4428] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4429\n",
            "-------------------------------\n",
            "[4429] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4430\n",
            "-------------------------------\n",
            "[4430] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4431\n",
            "-------------------------------\n",
            "[4431] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4432\n",
            "-------------------------------\n",
            "[4432] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4433\n",
            "-------------------------------\n",
            "[4433] loss: 0.000012\n",
            "      train_acc: 0.8427 val_acc:0.846847 \n",
            "      time: 0.007\n",
            "\n",
            "Epoch 4434\n",
            "-------------------------------\n",
            "[4434] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4435\n",
            "-------------------------------\n",
            "[4435] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4436\n",
            "-------------------------------\n",
            "[4436] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4437\n",
            "-------------------------------\n",
            "[4437] loss: 0.000012\n",
            "      train_acc: 0.8427 val_acc:0.846847 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4438\n",
            "-------------------------------\n",
            "[4438] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4439\n",
            "-------------------------------\n",
            "[4439] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4440\n",
            "-------------------------------\n",
            "[4440] loss: 0.000012\n",
            "      train_acc: 0.8427 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4441\n",
            "-------------------------------\n",
            "[4441] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4442\n",
            "-------------------------------\n",
            "[4442] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4443\n",
            "-------------------------------\n",
            "[4443] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4444\n",
            "-------------------------------\n",
            "[4444] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4445\n",
            "-------------------------------\n",
            "[4445] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4446\n",
            "-------------------------------\n",
            "[4446] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4447\n",
            "-------------------------------\n",
            "[4447] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4448\n",
            "-------------------------------\n",
            "[4448] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4449\n",
            "-------------------------------\n",
            "[4449] loss: 0.000012\n",
            "      train_acc: 0.8438 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4450\n",
            "-------------------------------\n",
            "[4450] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4451\n",
            "-------------------------------\n",
            "[4451] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4452\n",
            "-------------------------------\n",
            "[4452] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4453\n",
            "-------------------------------\n",
            "[4453] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4454\n",
            "-------------------------------\n",
            "[4454] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4455\n",
            "-------------------------------\n",
            "[4455] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4456\n",
            "-------------------------------\n",
            "[4456] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4457\n",
            "-------------------------------\n",
            "[4457] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4458\n",
            "-------------------------------\n",
            "[4458] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4459\n",
            "-------------------------------\n",
            "[4459] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4460\n",
            "-------------------------------\n",
            "[4460] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4461\n",
            "-------------------------------\n",
            "[4461] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4462\n",
            "-------------------------------\n",
            "[4462] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4463\n",
            "-------------------------------\n",
            "[4463] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4464\n",
            "-------------------------------\n",
            "[4464] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4465\n",
            "-------------------------------\n",
            "[4465] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4466\n",
            "-------------------------------\n",
            "[4466] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4467\n",
            "-------------------------------\n",
            "[4467] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4468\n",
            "-------------------------------\n",
            "[4468] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4469\n",
            "-------------------------------\n",
            "[4469] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4470\n",
            "-------------------------------\n",
            "[4470] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4471\n",
            "-------------------------------\n",
            "[4471] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4472\n",
            "-------------------------------\n",
            "[4472] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4473\n",
            "-------------------------------\n",
            "[4473] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4474\n",
            "-------------------------------\n",
            "[4474] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4475\n",
            "-------------------------------\n",
            "[4475] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4476\n",
            "-------------------------------\n",
            "[4476] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4477\n",
            "-------------------------------\n",
            "[4477] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4478\n",
            "-------------------------------\n",
            "[4478] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4479\n",
            "-------------------------------\n",
            "[4479] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4480\n",
            "-------------------------------\n",
            "[4480] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4481\n",
            "-------------------------------\n",
            "[4481] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4482\n",
            "-------------------------------\n",
            "[4482] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4483\n",
            "-------------------------------\n",
            "[4483] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4484\n",
            "-------------------------------\n",
            "[4484] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4485\n",
            "-------------------------------\n",
            "[4485] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4486\n",
            "-------------------------------\n",
            "[4486] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4487\n",
            "-------------------------------\n",
            "[4487] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4488\n",
            "-------------------------------\n",
            "[4488] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4489\n",
            "-------------------------------\n",
            "[4489] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4490\n",
            "-------------------------------\n",
            "[4490] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4491\n",
            "-------------------------------\n",
            "[4491] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4492\n",
            "-------------------------------\n",
            "[4492] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4493\n",
            "-------------------------------\n",
            "[4493] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4494\n",
            "-------------------------------\n",
            "[4494] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4495\n",
            "-------------------------------\n",
            "[4495] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4496\n",
            "-------------------------------\n",
            "[4496] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4497\n",
            "-------------------------------\n",
            "[4497] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4498\n",
            "-------------------------------\n",
            "[4498] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4499\n",
            "-------------------------------\n",
            "[4499] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4500\n",
            "-------------------------------\n",
            "[4500] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4501\n",
            "-------------------------------\n",
            "[4501] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4502\n",
            "-------------------------------\n",
            "[4502] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.007\n",
            "\n",
            "Epoch 4503\n",
            "-------------------------------\n",
            "[4503] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4504\n",
            "-------------------------------\n",
            "[4504] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4505\n",
            "-------------------------------\n",
            "[4505] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4506\n",
            "-------------------------------\n",
            "[4506] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4507\n",
            "-------------------------------\n",
            "[4507] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4508\n",
            "-------------------------------\n",
            "[4508] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4509\n",
            "-------------------------------\n",
            "[4509] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4510\n",
            "-------------------------------\n",
            "[4510] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4511\n",
            "-------------------------------\n",
            "[4511] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4512\n",
            "-------------------------------\n",
            "[4512] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4513\n",
            "-------------------------------\n",
            "[4513] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4514\n",
            "-------------------------------\n",
            "[4514] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4515\n",
            "-------------------------------\n",
            "[4515] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4516\n",
            "-------------------------------\n",
            "[4516] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4517\n",
            "-------------------------------\n",
            "[4517] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4518\n",
            "-------------------------------\n",
            "[4518] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4519\n",
            "-------------------------------\n",
            "[4519] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4520\n",
            "-------------------------------\n",
            "[4520] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4521\n",
            "-------------------------------\n",
            "[4521] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4522\n",
            "-------------------------------\n",
            "[4522] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4523\n",
            "-------------------------------\n",
            "[4523] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4524\n",
            "-------------------------------\n",
            "[4524] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4525\n",
            "-------------------------------\n",
            "[4525] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4526\n",
            "-------------------------------\n",
            "[4526] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.009\n",
            "\n",
            "Epoch 4527\n",
            "-------------------------------\n",
            "[4527] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4528\n",
            "-------------------------------\n",
            "[4528] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4529\n",
            "-------------------------------\n",
            "[4529] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4530\n",
            "-------------------------------\n",
            "[4530] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4531\n",
            "-------------------------------\n",
            "[4531] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4532\n",
            "-------------------------------\n",
            "[4532] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4533\n",
            "-------------------------------\n",
            "[4533] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4534\n",
            "-------------------------------\n",
            "[4534] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4535\n",
            "-------------------------------\n",
            "[4535] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4536\n",
            "-------------------------------\n",
            "[4536] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4537\n",
            "-------------------------------\n",
            "[4537] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4538\n",
            "-------------------------------\n",
            "[4538] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4539\n",
            "-------------------------------\n",
            "[4539] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4540\n",
            "-------------------------------\n",
            "[4540] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4541\n",
            "-------------------------------\n",
            "[4541] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4542\n",
            "-------------------------------\n",
            "[4542] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4543\n",
            "-------------------------------\n",
            "[4543] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4544\n",
            "-------------------------------\n",
            "[4544] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4545\n",
            "-------------------------------\n",
            "[4545] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4546\n",
            "-------------------------------\n",
            "[4546] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4547\n",
            "-------------------------------\n",
            "[4547] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.008\n",
            "\n",
            "Epoch 4548\n",
            "-------------------------------\n",
            "[4548] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4549\n",
            "-------------------------------\n",
            "[4549] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4550\n",
            "-------------------------------\n",
            "[4550] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4551\n",
            "-------------------------------\n",
            "[4551] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4552\n",
            "-------------------------------\n",
            "[4552] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4553\n",
            "-------------------------------\n",
            "[4553] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4554\n",
            "-------------------------------\n",
            "[4554] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4555\n",
            "-------------------------------\n",
            "[4555] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4556\n",
            "-------------------------------\n",
            "[4556] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4557\n",
            "-------------------------------\n",
            "[4557] loss: 0.000012\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4558\n",
            "-------------------------------\n",
            "[4558] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4559\n",
            "-------------------------------\n",
            "[4559] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4560\n",
            "-------------------------------\n",
            "[4560] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4561\n",
            "-------------------------------\n",
            "[4561] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4562\n",
            "-------------------------------\n",
            "[4562] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4563\n",
            "-------------------------------\n",
            "[4563] loss: 0.000012\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4564\n",
            "-------------------------------\n",
            "[4564] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4565\n",
            "-------------------------------\n",
            "[4565] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4566\n",
            "-------------------------------\n",
            "[4566] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4567\n",
            "-------------------------------\n",
            "[4567] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4568\n",
            "-------------------------------\n",
            "[4568] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4569\n",
            "-------------------------------\n",
            "[4569] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4570\n",
            "-------------------------------\n",
            "[4570] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4571\n",
            "-------------------------------\n",
            "[4571] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.007\n",
            "\n",
            "Epoch 4572\n",
            "-------------------------------\n",
            "[4572] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4573\n",
            "-------------------------------\n",
            "[4573] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4574\n",
            "-------------------------------\n",
            "[4574] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4575\n",
            "-------------------------------\n",
            "[4575] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4576\n",
            "-------------------------------\n",
            "[4576] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4577\n",
            "-------------------------------\n",
            "[4577] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4578\n",
            "-------------------------------\n",
            "[4578] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4579\n",
            "-------------------------------\n",
            "[4579] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4580\n",
            "-------------------------------\n",
            "[4580] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4581\n",
            "-------------------------------\n",
            "[4581] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4582\n",
            "-------------------------------\n",
            "[4582] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4583\n",
            "-------------------------------\n",
            "[4583] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4584\n",
            "-------------------------------\n",
            "[4584] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4585\n",
            "-------------------------------\n",
            "[4585] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4586\n",
            "-------------------------------\n",
            "[4586] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4587\n",
            "-------------------------------\n",
            "[4587] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4588\n",
            "-------------------------------\n",
            "[4588] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4589\n",
            "-------------------------------\n",
            "[4589] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4590\n",
            "-------------------------------\n",
            "[4590] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4591\n",
            "-------------------------------\n",
            "[4591] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4592\n",
            "-------------------------------\n",
            "[4592] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4593\n",
            "-------------------------------\n",
            "[4593] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4594\n",
            "-------------------------------\n",
            "[4594] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4595\n",
            "-------------------------------\n",
            "[4595] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4596\n",
            "-------------------------------\n",
            "[4596] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4597\n",
            "-------------------------------\n",
            "[4597] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4598\n",
            "-------------------------------\n",
            "[4598] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4599\n",
            "-------------------------------\n",
            "[4599] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4600\n",
            "-------------------------------\n",
            "[4600] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4601\n",
            "-------------------------------\n",
            "[4601] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4602\n",
            "-------------------------------\n",
            "[4602] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4603\n",
            "-------------------------------\n",
            "[4603] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4604\n",
            "-------------------------------\n",
            "[4604] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4605\n",
            "-------------------------------\n",
            "[4605] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4606\n",
            "-------------------------------\n",
            "[4606] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4607\n",
            "-------------------------------\n",
            "[4607] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4608\n",
            "-------------------------------\n",
            "[4608] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4609\n",
            "-------------------------------\n",
            "[4609] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4610\n",
            "-------------------------------\n",
            "[4610] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4611\n",
            "-------------------------------\n",
            "[4611] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4612\n",
            "-------------------------------\n",
            "[4612] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4613\n",
            "-------------------------------\n",
            "[4613] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4614\n",
            "-------------------------------\n",
            "[4614] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4615\n",
            "-------------------------------\n",
            "[4615] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4616\n",
            "-------------------------------\n",
            "[4616] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4617\n",
            "-------------------------------\n",
            "[4617] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4618\n",
            "-------------------------------\n",
            "[4618] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4619\n",
            "-------------------------------\n",
            "[4619] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4620\n",
            "-------------------------------\n",
            "[4620] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4621\n",
            "-------------------------------\n",
            "[4621] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4622\n",
            "-------------------------------\n",
            "[4622] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4623\n",
            "-------------------------------\n",
            "[4623] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4624\n",
            "-------------------------------\n",
            "[4624] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4625\n",
            "-------------------------------\n",
            "[4625] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4626\n",
            "-------------------------------\n",
            "[4626] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4627\n",
            "-------------------------------\n",
            "[4627] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4628\n",
            "-------------------------------\n",
            "[4628] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4629\n",
            "-------------------------------\n",
            "[4629] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4630\n",
            "-------------------------------\n",
            "[4630] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4631\n",
            "-------------------------------\n",
            "[4631] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4632\n",
            "-------------------------------\n",
            "[4632] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4633\n",
            "-------------------------------\n",
            "[4633] loss: 0.000012\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4634\n",
            "-------------------------------\n",
            "[4634] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4635\n",
            "-------------------------------\n",
            "[4635] loss: 0.000012\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4636\n",
            "-------------------------------\n",
            "[4636] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4637\n",
            "-------------------------------\n",
            "[4637] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.009\n",
            "\n",
            "Epoch 4638\n",
            "-------------------------------\n",
            "[4638] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4639\n",
            "-------------------------------\n",
            "[4639] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4640\n",
            "-------------------------------\n",
            "[4640] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4641\n",
            "-------------------------------\n",
            "[4641] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4642\n",
            "-------------------------------\n",
            "[4642] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4643\n",
            "-------------------------------\n",
            "[4643] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4644\n",
            "-------------------------------\n",
            "[4644] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4645\n",
            "-------------------------------\n",
            "[4645] loss: 0.000012\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4646\n",
            "-------------------------------\n",
            "[4646] loss: 0.000012\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4647\n",
            "-------------------------------\n",
            "[4647] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4648\n",
            "-------------------------------\n",
            "[4648] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4649\n",
            "-------------------------------\n",
            "[4649] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.007\n",
            "\n",
            "Epoch 4650\n",
            "-------------------------------\n",
            "[4650] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4651\n",
            "-------------------------------\n",
            "[4651] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4652\n",
            "-------------------------------\n",
            "[4652] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4653\n",
            "-------------------------------\n",
            "[4653] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4654\n",
            "-------------------------------\n",
            "[4654] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4655\n",
            "-------------------------------\n",
            "[4655] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4656\n",
            "-------------------------------\n",
            "[4656] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4657\n",
            "-------------------------------\n",
            "[4657] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4658\n",
            "-------------------------------\n",
            "[4658] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4659\n",
            "-------------------------------\n",
            "[4659] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4660\n",
            "-------------------------------\n",
            "[4660] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4661\n",
            "-------------------------------\n",
            "[4661] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4662\n",
            "-------------------------------\n",
            "[4662] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4663\n",
            "-------------------------------\n",
            "[4663] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4664\n",
            "-------------------------------\n",
            "[4664] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4665\n",
            "-------------------------------\n",
            "[4665] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4666\n",
            "-------------------------------\n",
            "[4666] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4667\n",
            "-------------------------------\n",
            "[4667] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4668\n",
            "-------------------------------\n",
            "[4668] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4669\n",
            "-------------------------------\n",
            "[4669] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4670\n",
            "-------------------------------\n",
            "[4670] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4671\n",
            "-------------------------------\n",
            "[4671] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4672\n",
            "-------------------------------\n",
            "[4672] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4673\n",
            "-------------------------------\n",
            "[4673] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4674\n",
            "-------------------------------\n",
            "[4674] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4675\n",
            "-------------------------------\n",
            "[4675] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4676\n",
            "-------------------------------\n",
            "[4676] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4677\n",
            "-------------------------------\n",
            "[4677] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4678\n",
            "-------------------------------\n",
            "[4678] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4679\n",
            "-------------------------------\n",
            "[4679] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4680\n",
            "-------------------------------\n",
            "[4680] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4681\n",
            "-------------------------------\n",
            "[4681] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4682\n",
            "-------------------------------\n",
            "[4682] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4683\n",
            "-------------------------------\n",
            "[4683] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4684\n",
            "-------------------------------\n",
            "[4684] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.008\n",
            "\n",
            "Epoch 4685\n",
            "-------------------------------\n",
            "[4685] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4686\n",
            "-------------------------------\n",
            "[4686] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4687\n",
            "-------------------------------\n",
            "[4687] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4688\n",
            "-------------------------------\n",
            "[4688] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.010\n",
            "\n",
            "Epoch 4689\n",
            "-------------------------------\n",
            "[4689] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4690\n",
            "-------------------------------\n",
            "[4690] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4691\n",
            "-------------------------------\n",
            "[4691] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4692\n",
            "-------------------------------\n",
            "[4692] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4693\n",
            "-------------------------------\n",
            "[4693] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4694\n",
            "-------------------------------\n",
            "[4694] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4695\n",
            "-------------------------------\n",
            "[4695] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4696\n",
            "-------------------------------\n",
            "[4696] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4697\n",
            "-------------------------------\n",
            "[4697] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4698\n",
            "-------------------------------\n",
            "[4698] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4699\n",
            "-------------------------------\n",
            "[4699] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4700\n",
            "-------------------------------\n",
            "[4700] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4701\n",
            "-------------------------------\n",
            "[4701] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4702\n",
            "-------------------------------\n",
            "[4702] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.007\n",
            "\n",
            "Epoch 4703\n",
            "-------------------------------\n",
            "[4703] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4704\n",
            "-------------------------------\n",
            "[4704] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4705\n",
            "-------------------------------\n",
            "[4705] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4706\n",
            "-------------------------------\n",
            "[4706] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4707\n",
            "-------------------------------\n",
            "[4707] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4708\n",
            "-------------------------------\n",
            "[4708] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4709\n",
            "-------------------------------\n",
            "[4709] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4710\n",
            "-------------------------------\n",
            "[4710] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4711\n",
            "-------------------------------\n",
            "[4711] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4712\n",
            "-------------------------------\n",
            "[4712] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4713\n",
            "-------------------------------\n",
            "[4713] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4714\n",
            "-------------------------------\n",
            "[4714] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4715\n",
            "-------------------------------\n",
            "[4715] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4716\n",
            "-------------------------------\n",
            "[4716] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4717\n",
            "-------------------------------\n",
            "[4717] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4718\n",
            "-------------------------------\n",
            "[4718] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4719\n",
            "-------------------------------\n",
            "[4719] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4720\n",
            "-------------------------------\n",
            "[4720] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4721\n",
            "-------------------------------\n",
            "[4721] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4722\n",
            "-------------------------------\n",
            "[4722] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4723\n",
            "-------------------------------\n",
            "[4723] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4724\n",
            "-------------------------------\n",
            "[4724] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4725\n",
            "-------------------------------\n",
            "[4725] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4726\n",
            "-------------------------------\n",
            "[4726] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4727\n",
            "-------------------------------\n",
            "[4727] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4728\n",
            "-------------------------------\n",
            "[4728] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.008\n",
            "\n",
            "Epoch 4729\n",
            "-------------------------------\n",
            "[4729] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4730\n",
            "-------------------------------\n",
            "[4730] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4731\n",
            "-------------------------------\n",
            "[4731] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4732\n",
            "-------------------------------\n",
            "[4732] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4733\n",
            "-------------------------------\n",
            "[4733] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4734\n",
            "-------------------------------\n",
            "[4734] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4735\n",
            "-------------------------------\n",
            "[4735] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4736\n",
            "-------------------------------\n",
            "[4736] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4737\n",
            "-------------------------------\n",
            "[4737] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4738\n",
            "-------------------------------\n",
            "[4738] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4739\n",
            "-------------------------------\n",
            "[4739] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4740\n",
            "-------------------------------\n",
            "[4740] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4741\n",
            "-------------------------------\n",
            "[4741] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4742\n",
            "-------------------------------\n",
            "[4742] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4743\n",
            "-------------------------------\n",
            "[4743] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4744\n",
            "-------------------------------\n",
            "[4744] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4745\n",
            "-------------------------------\n",
            "[4745] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4746\n",
            "-------------------------------\n",
            "[4746] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4747\n",
            "-------------------------------\n",
            "[4747] loss: 0.000011\n",
            "      train_acc: 0.8360 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4748\n",
            "-------------------------------\n",
            "[4748] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4749\n",
            "-------------------------------\n",
            "[4749] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4750\n",
            "-------------------------------\n",
            "[4750] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.855856 \n",
            "      time: 0.007\n",
            "\n",
            "Epoch 4751\n",
            "-------------------------------\n",
            "[4751] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4752\n",
            "-------------------------------\n",
            "[4752] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4753\n",
            "-------------------------------\n",
            "[4753] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4754\n",
            "-------------------------------\n",
            "[4754] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4755\n",
            "-------------------------------\n",
            "[4755] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4756\n",
            "-------------------------------\n",
            "[4756] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4757\n",
            "-------------------------------\n",
            "[4757] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4758\n",
            "-------------------------------\n",
            "[4758] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.008\n",
            "\n",
            "Epoch 4759\n",
            "-------------------------------\n",
            "[4759] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4760\n",
            "-------------------------------\n",
            "[4760] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4761\n",
            "-------------------------------\n",
            "[4761] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4762\n",
            "-------------------------------\n",
            "[4762] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4763\n",
            "-------------------------------\n",
            "[4763] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4764\n",
            "-------------------------------\n",
            "[4764] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4765\n",
            "-------------------------------\n",
            "[4765] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4766\n",
            "-------------------------------\n",
            "[4766] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.007\n",
            "\n",
            "Epoch 4767\n",
            "-------------------------------\n",
            "[4767] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4768\n",
            "-------------------------------\n",
            "[4768] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4769\n",
            "-------------------------------\n",
            "[4769] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4770\n",
            "-------------------------------\n",
            "[4770] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4771\n",
            "-------------------------------\n",
            "[4771] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.010\n",
            "\n",
            "Epoch 4772\n",
            "-------------------------------\n",
            "[4772] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4773\n",
            "-------------------------------\n",
            "[4773] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4774\n",
            "-------------------------------\n",
            "[4774] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4775\n",
            "-------------------------------\n",
            "[4775] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4776\n",
            "-------------------------------\n",
            "[4776] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4777\n",
            "-------------------------------\n",
            "[4777] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4778\n",
            "-------------------------------\n",
            "[4778] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4779\n",
            "-------------------------------\n",
            "[4779] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4780\n",
            "-------------------------------\n",
            "[4780] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4781\n",
            "-------------------------------\n",
            "[4781] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4782\n",
            "-------------------------------\n",
            "[4782] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4783\n",
            "-------------------------------\n",
            "[4783] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4784\n",
            "-------------------------------\n",
            "[4784] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4785\n",
            "-------------------------------\n",
            "[4785] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4786\n",
            "-------------------------------\n",
            "[4786] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4787\n",
            "-------------------------------\n",
            "[4787] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4788\n",
            "-------------------------------\n",
            "[4788] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4789\n",
            "-------------------------------\n",
            "[4789] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4790\n",
            "-------------------------------\n",
            "[4790] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4791\n",
            "-------------------------------\n",
            "[4791] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4792\n",
            "-------------------------------\n",
            "[4792] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4793\n",
            "-------------------------------\n",
            "[4793] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4794\n",
            "-------------------------------\n",
            "[4794] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4795\n",
            "-------------------------------\n",
            "[4795] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4796\n",
            "-------------------------------\n",
            "[4796] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4797\n",
            "-------------------------------\n",
            "[4797] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4798\n",
            "-------------------------------\n",
            "[4798] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4799\n",
            "-------------------------------\n",
            "[4799] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4800\n",
            "-------------------------------\n",
            "[4800] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4801\n",
            "-------------------------------\n",
            "[4801] loss: 0.000011\n",
            "      train_acc: 0.8483 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4802\n",
            "-------------------------------\n",
            "[4802] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4803\n",
            "-------------------------------\n",
            "[4803] loss: 0.000011\n",
            "      train_acc: 0.8494 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4804\n",
            "-------------------------------\n",
            "[4804] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4805\n",
            "-------------------------------\n",
            "[4805] loss: 0.000011\n",
            "      train_acc: 0.8494 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4806\n",
            "-------------------------------\n",
            "[4806] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4807\n",
            "-------------------------------\n",
            "[4807] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4808\n",
            "-------------------------------\n",
            "[4808] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4809\n",
            "-------------------------------\n",
            "[4809] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4810\n",
            "-------------------------------\n",
            "[4810] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4811\n",
            "-------------------------------\n",
            "[4811] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4812\n",
            "-------------------------------\n",
            "[4812] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4813\n",
            "-------------------------------\n",
            "[4813] loss: 0.000011\n",
            "      train_acc: 0.8360 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4814\n",
            "-------------------------------\n",
            "[4814] loss: 0.000011\n",
            "      train_acc: 0.8494 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4815\n",
            "-------------------------------\n",
            "[4815] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4816\n",
            "-------------------------------\n",
            "[4816] loss: 0.000011\n",
            "      train_acc: 0.8483 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4817\n",
            "-------------------------------\n",
            "[4817] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.008\n",
            "\n",
            "Epoch 4818\n",
            "-------------------------------\n",
            "[4818] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4819\n",
            "-------------------------------\n",
            "[4819] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4820\n",
            "-------------------------------\n",
            "[4820] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4821\n",
            "-------------------------------\n",
            "[4821] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4822\n",
            "-------------------------------\n",
            "[4822] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4823\n",
            "-------------------------------\n",
            "[4823] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4824\n",
            "-------------------------------\n",
            "[4824] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4825\n",
            "-------------------------------\n",
            "[4825] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4826\n",
            "-------------------------------\n",
            "[4826] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4827\n",
            "-------------------------------\n",
            "[4827] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4828\n",
            "-------------------------------\n",
            "[4828] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4829\n",
            "-------------------------------\n",
            "[4829] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4830\n",
            "-------------------------------\n",
            "[4830] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4831\n",
            "-------------------------------\n",
            "[4831] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4832\n",
            "-------------------------------\n",
            "[4832] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4833\n",
            "-------------------------------\n",
            "[4833] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4834\n",
            "-------------------------------\n",
            "[4834] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4835\n",
            "-------------------------------\n",
            "[4835] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4836\n",
            "-------------------------------\n",
            "[4836] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4837\n",
            "-------------------------------\n",
            "[4837] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4838\n",
            "-------------------------------\n",
            "[4838] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4839\n",
            "-------------------------------\n",
            "[4839] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4840\n",
            "-------------------------------\n",
            "[4840] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.010\n",
            "\n",
            "Epoch 4841\n",
            "-------------------------------\n",
            "[4841] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4842\n",
            "-------------------------------\n",
            "[4842] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4843\n",
            "-------------------------------\n",
            "[4843] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4844\n",
            "-------------------------------\n",
            "[4844] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4845\n",
            "-------------------------------\n",
            "[4845] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4846\n",
            "-------------------------------\n",
            "[4846] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4847\n",
            "-------------------------------\n",
            "[4847] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4848\n",
            "-------------------------------\n",
            "[4848] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4849\n",
            "-------------------------------\n",
            "[4849] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4850\n",
            "-------------------------------\n",
            "[4850] loss: 0.000011\n",
            "      train_acc: 0.8483 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4851\n",
            "-------------------------------\n",
            "[4851] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4852\n",
            "-------------------------------\n",
            "[4852] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4853\n",
            "-------------------------------\n",
            "[4853] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4854\n",
            "-------------------------------\n",
            "[4854] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4855\n",
            "-------------------------------\n",
            "[4855] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4856\n",
            "-------------------------------\n",
            "[4856] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4857\n",
            "-------------------------------\n",
            "[4857] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4858\n",
            "-------------------------------\n",
            "[4858] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4859\n",
            "-------------------------------\n",
            "[4859] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4860\n",
            "-------------------------------\n",
            "[4860] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4861\n",
            "-------------------------------\n",
            "[4861] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4862\n",
            "-------------------------------\n",
            "[4862] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4863\n",
            "-------------------------------\n",
            "[4863] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.010\n",
            "\n",
            "Epoch 4864\n",
            "-------------------------------\n",
            "[4864] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4865\n",
            "-------------------------------\n",
            "[4865] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4866\n",
            "-------------------------------\n",
            "[4866] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4867\n",
            "-------------------------------\n",
            "[4867] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4868\n",
            "-------------------------------\n",
            "[4868] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4869\n",
            "-------------------------------\n",
            "[4869] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4870\n",
            "-------------------------------\n",
            "[4870] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4871\n",
            "-------------------------------\n",
            "[4871] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4872\n",
            "-------------------------------\n",
            "[4872] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4873\n",
            "-------------------------------\n",
            "[4873] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4874\n",
            "-------------------------------\n",
            "[4874] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4875\n",
            "-------------------------------\n",
            "[4875] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4876\n",
            "-------------------------------\n",
            "[4876] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4877\n",
            "-------------------------------\n",
            "[4877] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4878\n",
            "-------------------------------\n",
            "[4878] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4879\n",
            "-------------------------------\n",
            "[4879] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4880\n",
            "-------------------------------\n",
            "[4880] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4881\n",
            "-------------------------------\n",
            "[4881] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4882\n",
            "-------------------------------\n",
            "[4882] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4883\n",
            "-------------------------------\n",
            "[4883] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4884\n",
            "-------------------------------\n",
            "[4884] loss: 0.000011\n",
            "      train_acc: 0.8483 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4885\n",
            "-------------------------------\n",
            "[4885] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4886\n",
            "-------------------------------\n",
            "[4886] loss: 0.000011\n",
            "      train_acc: 0.8494 val_acc:0.855856 \n",
            "      time: 0.008\n",
            "\n",
            "Epoch 4887\n",
            "-------------------------------\n",
            "[4887] loss: 0.000011\n",
            "      train_acc: 0.8360 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4888\n",
            "-------------------------------\n",
            "[4888] loss: 0.000011\n",
            "      train_acc: 0.8483 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4889\n",
            "-------------------------------\n",
            "[4889] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4890\n",
            "-------------------------------\n",
            "[4890] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4891\n",
            "-------------------------------\n",
            "[4891] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4892\n",
            "-------------------------------\n",
            "[4892] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4893\n",
            "-------------------------------\n",
            "[4893] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4894\n",
            "-------------------------------\n",
            "[4894] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4895\n",
            "-------------------------------\n",
            "[4895] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4896\n",
            "-------------------------------\n",
            "[4896] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.007\n",
            "\n",
            "Epoch 4897\n",
            "-------------------------------\n",
            "[4897] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4898\n",
            "-------------------------------\n",
            "[4898] loss: 0.000011\n",
            "      train_acc: 0.8360 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4899\n",
            "-------------------------------\n",
            "[4899] loss: 0.000011\n",
            "      train_acc: 0.8483 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4900\n",
            "-------------------------------\n",
            "[4900] loss: 0.000011\n",
            "      train_acc: 0.8348 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4901\n",
            "-------------------------------\n",
            "[4901] loss: 0.000011\n",
            "      train_acc: 0.8483 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4902\n",
            "-------------------------------\n",
            "[4902] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4903\n",
            "-------------------------------\n",
            "[4903] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4904\n",
            "-------------------------------\n",
            "[4904] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4905\n",
            "-------------------------------\n",
            "[4905] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4906\n",
            "-------------------------------\n",
            "[4906] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4907\n",
            "-------------------------------\n",
            "[4907] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4908\n",
            "-------------------------------\n",
            "[4908] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4909\n",
            "-------------------------------\n",
            "[4909] loss: 0.000011\n",
            "      train_acc: 0.8360 val_acc:0.846847 \n",
            "      time: 0.010\n",
            "\n",
            "Epoch 4910\n",
            "-------------------------------\n",
            "[4910] loss: 0.000011\n",
            "      train_acc: 0.8483 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4911\n",
            "-------------------------------\n",
            "[4911] loss: 0.000011\n",
            "      train_acc: 0.8360 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4912\n",
            "-------------------------------\n",
            "[4912] loss: 0.000011\n",
            "      train_acc: 0.8483 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4913\n",
            "-------------------------------\n",
            "[4913] loss: 0.000011\n",
            "      train_acc: 0.8360 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4914\n",
            "-------------------------------\n",
            "[4914] loss: 0.000011\n",
            "      train_acc: 0.8483 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4915\n",
            "-------------------------------\n",
            "[4915] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4916\n",
            "-------------------------------\n",
            "[4916] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4917\n",
            "-------------------------------\n",
            "[4917] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4918\n",
            "-------------------------------\n",
            "[4918] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4919\n",
            "-------------------------------\n",
            "[4919] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4920\n",
            "-------------------------------\n",
            "[4920] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4921\n",
            "-------------------------------\n",
            "[4921] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4922\n",
            "-------------------------------\n",
            "[4922] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4923\n",
            "-------------------------------\n",
            "[4923] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4924\n",
            "-------------------------------\n",
            "[4924] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4925\n",
            "-------------------------------\n",
            "[4925] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4926\n",
            "-------------------------------\n",
            "[4926] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4927\n",
            "-------------------------------\n",
            "[4927] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4928\n",
            "-------------------------------\n",
            "[4928] loss: 0.000011\n",
            "      train_acc: 0.8371 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4929\n",
            "-------------------------------\n",
            "[4929] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4930\n",
            "-------------------------------\n",
            "[4930] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4931\n",
            "-------------------------------\n",
            "[4931] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4932\n",
            "-------------------------------\n",
            "[4932] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4933\n",
            "-------------------------------\n",
            "[4933] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4934\n",
            "-------------------------------\n",
            "[4934] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4935\n",
            "-------------------------------\n",
            "[4935] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4936\n",
            "-------------------------------\n",
            "[4936] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4937\n",
            "-------------------------------\n",
            "[4937] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4938\n",
            "-------------------------------\n",
            "[4938] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4939\n",
            "-------------------------------\n",
            "[4939] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4940\n",
            "-------------------------------\n",
            "[4940] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4941\n",
            "-------------------------------\n",
            "[4941] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4942\n",
            "-------------------------------\n",
            "[4942] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4943\n",
            "-------------------------------\n",
            "[4943] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4944\n",
            "-------------------------------\n",
            "[4944] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4945\n",
            "-------------------------------\n",
            "[4945] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4946\n",
            "-------------------------------\n",
            "[4946] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4947\n",
            "-------------------------------\n",
            "[4947] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4948\n",
            "-------------------------------\n",
            "[4948] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4949\n",
            "-------------------------------\n",
            "[4949] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4950\n",
            "-------------------------------\n",
            "[4950] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4951\n",
            "-------------------------------\n",
            "[4951] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4952\n",
            "-------------------------------\n",
            "[4952] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4953\n",
            "-------------------------------\n",
            "[4953] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4954\n",
            "-------------------------------\n",
            "[4954] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4955\n",
            "-------------------------------\n",
            "[4955] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.855856 \n",
            "      time: 0.006\n",
            "\n",
            "Epoch 4956\n",
            "-------------------------------\n",
            "[4956] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4957\n",
            "-------------------------------\n",
            "[4957] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4958\n",
            "-------------------------------\n",
            "[4958] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4959\n",
            "-------------------------------\n",
            "[4959] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4960\n",
            "-------------------------------\n",
            "[4960] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4961\n",
            "-------------------------------\n",
            "[4961] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4962\n",
            "-------------------------------\n",
            "[4962] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4963\n",
            "-------------------------------\n",
            "[4963] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4964\n",
            "-------------------------------\n",
            "[4964] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4965\n",
            "-------------------------------\n",
            "[4965] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4966\n",
            "-------------------------------\n",
            "[4966] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.855856 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4967\n",
            "-------------------------------\n",
            "[4967] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4968\n",
            "-------------------------------\n",
            "[4968] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4969\n",
            "-------------------------------\n",
            "[4969] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4970\n",
            "-------------------------------\n",
            "[4970] loss: 0.000011\n",
            "      train_acc: 0.8360 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4971\n",
            "-------------------------------\n",
            "[4971] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4972\n",
            "-------------------------------\n",
            "[4972] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4973\n",
            "-------------------------------\n",
            "[4973] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4974\n",
            "-------------------------------\n",
            "[4974] loss: 0.000011\n",
            "      train_acc: 0.8360 val_acc:0.837838 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4975\n",
            "-------------------------------\n",
            "[4975] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4976\n",
            "-------------------------------\n",
            "[4976] loss: 0.000011\n",
            "      train_acc: 0.8348 val_acc:0.837838 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4977\n",
            "-------------------------------\n",
            "[4977] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.846847 \n",
            "      time: 0.010\n",
            "\n",
            "Epoch 4978\n",
            "-------------------------------\n",
            "[4978] loss: 0.000011\n",
            "      train_acc: 0.8348 val_acc:0.837838 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4979\n",
            "-------------------------------\n",
            "[4979] loss: 0.000011\n",
            "      train_acc: 0.8483 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4980\n",
            "-------------------------------\n",
            "[4980] loss: 0.000011\n",
            "      train_acc: 0.8348 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4981\n",
            "-------------------------------\n",
            "[4981] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4982\n",
            "-------------------------------\n",
            "[4982] loss: 0.000011\n",
            "      train_acc: 0.8348 val_acc:0.837838 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4983\n",
            "-------------------------------\n",
            "[4983] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4984\n",
            "-------------------------------\n",
            "[4984] loss: 0.000011\n",
            "      train_acc: 0.8348 val_acc:0.837838 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4985\n",
            "-------------------------------\n",
            "[4985] loss: 0.000011\n",
            "      train_acc: 0.8472 val_acc:0.846847 \n",
            "      time: 0.005\n",
            "\n",
            "Epoch 4986\n",
            "-------------------------------\n",
            "[4986] loss: 0.000011\n",
            "      train_acc: 0.8337 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4987\n",
            "-------------------------------\n",
            "[4987] loss: 0.000011\n",
            "      train_acc: 0.8449 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4988\n",
            "-------------------------------\n",
            "[4988] loss: 0.000011\n",
            "      train_acc: 0.8382 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4989\n",
            "-------------------------------\n",
            "[4989] loss: 0.000011\n",
            "      train_acc: 0.8461 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4990\n",
            "-------------------------------\n",
            "[4990] loss: 0.000011\n",
            "      train_acc: 0.8416 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4991\n",
            "-------------------------------\n",
            "[4991] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4992\n",
            "-------------------------------\n",
            "[4992] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4993\n",
            "-------------------------------\n",
            "[4993] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4994\n",
            "-------------------------------\n",
            "[4994] loss: 0.000011\n",
            "      train_acc: 0.8438 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4995\n",
            "-------------------------------\n",
            "[4995] loss: 0.000011\n",
            "      train_acc: 0.8404 val_acc:0.837838 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4996\n",
            "-------------------------------\n",
            "[4996] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4997\n",
            "-------------------------------\n",
            "[4997] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.837838 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4998\n",
            "-------------------------------\n",
            "[4998] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 4999\n",
            "-------------------------------\n",
            "[4999] loss: 0.000011\n",
            "      train_acc: 0.8393 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n",
            "Epoch 5000\n",
            "-------------------------------\n",
            "[5000] loss: 0.000011\n",
            "      train_acc: 0.8427 val_acc:0.846847 \n",
            "      time: 0.004\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1, epochs+1), metrics['total_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n",
        "plt.title('Train loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "an_M1eUpbW4y",
        "outputId": "da935015-c2b9-4c00-ae24-fb9a364acd7e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1, epochs+1), metrics['val_acc'], label='val acc')\n",
        "plt.plot(range(1, epochs+1), metrics['train_acc'], label='train acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ZOjOdjBKbXbq",
        "outputId": "1e6ff0c7-7306-4a0e-f06d-0984a4fa5a37"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "accuracy = BinaryAccuracy(threshold=0.5).to(device)\n",
        "test_output = model(test_features.to(device), test_a_norm.to(device), test_batch_idx.to(device))\n",
        "test_pred = nn.Sigmoid()(test_output)"
      ],
      "metadata": {
        "id": "2MGFGG7Ibb3A"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Test accuracy: {accuracy(test_pred, test_labels.to(device)).item():.4f}')\n",
        "print(f'Val accuracy: {metrics[\"val_acc\"][-1]:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYqjsZu5b_Wf",
        "outputId": "5b3d0b6a-9c21-426a-bc06-612e58ad792a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.7500\n",
            "Val accuracy: 0.8468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proteins can be long and complex, and they have functional domains = parts of the whole sequence that are important because they have a certain 3D structure that allows them to perform a specific function. But it is hard to tell why this would result in the specific loss curve shape we are seeing."
      ],
      "metadata": {
        "id": "dL_LpdI5vvjY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvmAfDNMcnKD"
      },
      "source": [
        "## The end\n",
        "\n",
        "If you have made it all the way here successfully, congratulations! 🎉 \n",
        "\n",
        "You have implemented your own GCN and tested it on a node classification task, and a more challenging classification task over multiple graphs.\n",
        "\n",
        "We hope you can use this knowledge to apply GCNs not only to the tasks described here, but other applications where data can be modeled as a graph.\n",
        "\n",
        "If you are interested in applying graph neural networks to larger graphs, or try newer architectures, you can dive deeper into [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/), a library with fast implementations for a wide range of architectures. It also comes with custom code that takes care of aspects that you dealt with manually for this assignment, like a more efficient implementation of the adjacency matrix multiplication via message-passing methods, and Data Loaders that relieve you from having to build block diagonal sparse matrices.\n",
        "\n",
        "You can also check the [Deep Graph Library](https://docs.dgl.ai/) another powerful library for deep learning on graphs which also integrates with other backends like TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7QhyAMms8-L"
      },
      "source": [
        "# Grading (10pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juIdxXhos-mV"
      },
      "source": [
        "- Question 1: 0.25pt \n",
        "- Question 2: 0.25pt \n",
        "- Question 3: 0.5pt \n",
        "- Question 4: 0.25pt \n",
        "- Question 5: 0.5pt \n",
        "- Question 6: 0.5pt \n",
        "- Question 7: 0.5pt \n",
        "- Question 8: 0.5pt \n",
        "- Question 9: 1.5pt \n",
        "- Question 10: 0.5pt \n",
        "- Question 11: 0.25pt \n",
        "- Question 12: 0.5pt \n",
        "- Question 13: 0.5pt \n",
        "- Question 14: 1pt\n",
        "- Question 15: 1pt\n",
        "- Question 16: 1.5pt"
      ]
    }
  ]
}